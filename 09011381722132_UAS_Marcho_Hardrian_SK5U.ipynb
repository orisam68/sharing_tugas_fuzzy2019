{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Prostate Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jaringan saraf tiruan (JST) (Bahasa Inggris: artificial neural network (ANN), atau juga disebut simulated neural network (SNN), atau umumnya hanya disebut neural network (NN)), adalah jaringan dari sekelompok unit pemroses kecil yang dimodelkan berdasarkan sistem saraf manusia. \n",
    "\n",
    "JST merupakan sistem adaptif yang dapat mengubah strukturnya untuk memecahkan masalah berdasarkan informasi eksternal maupun internal yang mengalir melalui jaringan tersebut. Oleh karena sifatnya yang adaptif, JST juga sering disebut dengan jaringan adaptif. \n",
    "\n",
    "\n",
    "Secara sederhana, JST adalah sebuah alat pemodelan data statistik non-linier. JST dapat digunakan untuk memodelkan hubungan yang kompleks antara input dan output untuk menemukan pola-pola pada data. Menurut suatu teorema yang disebut \"teorema penaksiran universal\", JST dengan minimal sebuah lapis tersembunyi dengan fungsi aktivasi non-linear dapat memodelkan seluruh fungsi terukur Boreal apapun dari suatu dimensi ke dimensi lainnya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prostate Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kanker prostat adalah pertumbuhan sel-sel secara tidak terkendali dalam kelenjar prostat. Prostat adalah kelenjar kecil di panggul pria yang merupakan bagian dari sistem reproduksi. Prostat berada di bawah kandung kemih di depan rektum. Kelenjar prostat mengelilingi uretra, yaitu saluran yang membawa urine dari kandung kemih ke penis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kanker prostat mungkin saja tidak menimbulkan gejala apa pun pada tahap awal. Gejala kanker prostat akan muncul ketika prostat terlalu besar atau membengkak dan mulai memengaruhi uretra. Beberapa tanda dan gejala yang muncul ketika ini terjadi adalah:\n",
    "\n",
    "    -Lebih sering buang air kecil, terutama saat malam hari\n",
    "    -Merasa nyeri atau panas pada penis saat buang air kecil atau ejakulasi\n",
    "    -Merasa kandung kemih selalu penuh\n",
    "    -Darah dalam urine atau air mani\n",
    "    -Tekanan saat mengeluarkan urine berkurang\n",
    "    \n",
    "Biasanya, tanda maupun gejala kanker prostat akan muncul ketika kanker sudah menyebar keluar dari prostat. Tapi gejala-gejala di atas tidak selalu disebabkan oleh kanker prostat. Kondisi di atas bisa saja disebabkan oleh infeksi saluran kencing.\n",
    "Hingga kini, penyebab munculnya kanker prostat masih belum diketahui. Tapi faktor keturunan atau genetik dan usia seseorang bisa meningkatkan risiko munculnya kanker prostat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Dataset yang digunakan harus dipanggil/dimasukan kesebuah variable agar bisa ditampung dan digunakan\n",
    "\n",
    "    -Data = variable yang menampung dataset yang digunakan untuk melakukan perhitungan Neural Network\n",
    "    -dataset_variable = variable digunakan untuk menampung data yang memiliki label, dan hanya untuk melihat keterangan header dari data set\n",
    "    -label_variable = variable yang digunakan untuk melihat label pada setiap feature\n",
    "    \n",
    "dataset yang digunakan memiliki 9 variable yang dimana \n",
    "    - 8 variable sebagai input\n",
    "    - 1 variable sebagai label\n",
    "    \n",
    "Variable Feature antara lain\n",
    "\n",
    "    1.Radius\n",
    "    2.Texture\n",
    "    3.Perimeter\n",
    "    4.area\n",
    "    5.Smoothness\n",
    "    6.Compactness\n",
    "    7.Synmmetry\n",
    "    8. Fractal Dimension\n",
    "    \n",
    " ID hanya Nomor Urut dari Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c3437ababbff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cancer1.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdataset_variable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/asus/Desktop/Semester 5/AI/prostate_cancer-master/prostate_cancer-master/Prostate_cancer.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlabel_variable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/asus/Desktop/Semester 5/AI/prostate_cancer-master/Label.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# split into input (X) and output (Y) variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows)\u001b[0m\n\u001b[0;32m   1139\u001b[0m         \u001b[1;31m# converting the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1140\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1141\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_loadtxt_chunksize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1142\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m                 \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mread_data\u001b[1;34m(chunk_size)\u001b[0m\n\u001b[0;32m   1066\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1067\u001b[0m             \u001b[1;31m# Convert each value according to its column and store\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1068\u001b[1;33m             \u001b[0mitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1069\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m             \u001b[1;31m# Then pack it according to the dtype's nesting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1066\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1067\u001b[0m             \u001b[1;31m# Convert each value according to its column and store\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1068\u001b[1;33m             \u001b[0mitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1069\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m             \u001b[1;31m# Then pack it according to the dtype's nesting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mfloatconv\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    773\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'0x'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    774\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromhex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 775\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    776\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m     \u001b[0mtyp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'id'"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt(\"cancer1.csv\", delimiter=\",\")\n",
    "dataset_variable=pd.read_csv(\"C:/Users/asus/Desktop/Semester 5/AI/prostate_cancer-master/prostate_cancer-master/Prostate_cancer.csv\")\n",
    "label_variable = pd.read_csv(\"C:/Users/asus/Desktop/Semester 5/AI/prostate_cancer-master/Label.csv\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = data[:,0:8]\n",
    "Y = data[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>radius</th>\n",
       "      <th>texture</th>\n",
       "      <th>perimeter</th>\n",
       "      <th>area</th>\n",
       "      <th>smoothness</th>\n",
       "      <th>compactness</th>\n",
       "      <th>symmetry</th>\n",
       "      <th>fractal_dimension</th>\n",
       "      <th>diagnosis_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>151</td>\n",
       "      <td>954</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.079</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>133</td>\n",
       "      <td>1326</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.057</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>130</td>\n",
       "      <td>1203</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.060</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>78</td>\n",
       "      <td>386</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.097</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>135</td>\n",
       "      <td>1297</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.059</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>83</td>\n",
       "      <td>477</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.076</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>26</td>\n",
       "      <td>120</td>\n",
       "      <td>1040</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.057</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>90</td>\n",
       "      <td>578</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.075</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>24</td>\n",
       "      <td>88</td>\n",
       "      <td>520</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.074</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>11</td>\n",
       "      <td>84</td>\n",
       "      <td>476</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.082</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "      <td>103</td>\n",
       "      <td>798</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.057</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>104</td>\n",
       "      <td>781</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.061</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>132</td>\n",
       "      <td>1123</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.078</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>104</td>\n",
       "      <td>783</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.053</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>94</td>\n",
       "      <td>578</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.077</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>97</td>\n",
       "      <td>659</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.071</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>95</td>\n",
       "      <td>685</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.059</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>108</td>\n",
       "      <td>799</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.074</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "      <td>130</td>\n",
       "      <td>1260</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.000</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>87</td>\n",
       "      <td>566</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.058</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>86</td>\n",
       "      <td>520</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.068</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>60</td>\n",
       "      <td>274</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.069</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "      <td>27</td>\n",
       "      <td>103</td>\n",
       "      <td>704</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.070</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>137</td>\n",
       "      <td>1404</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.053</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>110</td>\n",
       "      <td>905</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.063</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>19</td>\n",
       "      <td>27</td>\n",
       "      <td>116</td>\n",
       "      <td>913</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.074</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>97</td>\n",
       "      <td>645</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.069</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>122</td>\n",
       "      <td>1094</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.057</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>102</td>\n",
       "      <td>732</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.065</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>115</td>\n",
       "      <td>955</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.061</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>71</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>124</td>\n",
       "      <td>1130</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.055</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>72</td>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>59</td>\n",
       "      <td>244</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.090</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>73</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "      <td>114</td>\n",
       "      <td>929</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.065</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>74</td>\n",
       "      <td>22</td>\n",
       "      <td>25</td>\n",
       "      <td>90</td>\n",
       "      <td>584</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.066</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>75</td>\n",
       "      <td>18</td>\n",
       "      <td>13</td>\n",
       "      <td>79</td>\n",
       "      <td>471</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.059</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>76</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>104</td>\n",
       "      <td>818</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.054</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>77</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>88</td>\n",
       "      <td>559</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.066</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>78</td>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "      <td>120</td>\n",
       "      <td>1006</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.067</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>79</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>144</td>\n",
       "      <td>1245</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.081</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>80</td>\n",
       "      <td>22</td>\n",
       "      <td>16</td>\n",
       "      <td>83</td>\n",
       "      <td>506</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.060</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>81</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>74</td>\n",
       "      <td>402</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.070</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>82</td>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "      <td>86</td>\n",
       "      <td>520</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.069</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>83</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>172</td>\n",
       "      <td>1878</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.068</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>84</td>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "      <td>129</td>\n",
       "      <td>1132</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.072</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>85</td>\n",
       "      <td>25</td>\n",
       "      <td>21</td>\n",
       "      <td>77</td>\n",
       "      <td>443</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.060</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>86</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>121</td>\n",
       "      <td>1075</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.060</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>87</td>\n",
       "      <td>19</td>\n",
       "      <td>26</td>\n",
       "      <td>94</td>\n",
       "      <td>648</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.056</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>88</td>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>122</td>\n",
       "      <td>1076</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.056</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>89</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>80</td>\n",
       "      <td>466</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.064</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>90</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>96</td>\n",
       "      <td>652</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.063</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>91</td>\n",
       "      <td>23</td>\n",
       "      <td>27</td>\n",
       "      <td>95</td>\n",
       "      <td>663</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.059</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>92</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>728</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.061</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>93</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>85</td>\n",
       "      <td>552</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.053</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>94</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>87</td>\n",
       "      <td>555</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.057</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>95</td>\n",
       "      <td>22</td>\n",
       "      <td>26</td>\n",
       "      <td>100</td>\n",
       "      <td>706</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.063</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>132</td>\n",
       "      <td>1264</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.056</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>78</td>\n",
       "      <td>451</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.066</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>19</td>\n",
       "      <td>27</td>\n",
       "      <td>62</td>\n",
       "      <td>295</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.069</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>21</td>\n",
       "      <td>24</td>\n",
       "      <td>74</td>\n",
       "      <td>413</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.066</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "      <td>27</td>\n",
       "      <td>94</td>\n",
       "      <td>643</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.064</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  radius  texture  perimeter  area  smoothness  compactness  symmetry  \\\n",
       "0     1      23       12        151   954       0.143        0.278     0.242   \n",
       "1     2       9       13        133  1326       0.143        0.079     0.181   \n",
       "2     3      21       27        130  1203       0.125        0.160     0.207   \n",
       "3     4      14       16         78   386       0.070        0.284     0.260   \n",
       "4     5       9       19        135  1297       0.141        0.133     0.181   \n",
       "5     6      25       25         83   477       0.128        0.170     0.209   \n",
       "6     7      16       26        120  1040       0.095        0.109     0.179   \n",
       "7     8      15       18         90   578       0.119        0.165     0.220   \n",
       "8     9      19       24         88   520       0.127        0.193     0.235   \n",
       "9    10      25       11         84   476       0.119        0.240     0.203   \n",
       "10   11      24       21        103   798       0.082        0.067     0.153   \n",
       "11   12      17       15        104   781       0.097        0.129     0.184   \n",
       "12   13      14       15        132  1123       0.097        0.000     0.240   \n",
       "13   14      12       22        104   783       0.084        0.100     0.185   \n",
       "14   15      12       13         94   578       0.113        0.229     0.207   \n",
       "15   16      22       19         97   659       0.114        0.160     0.230   \n",
       "16   17      10       16         95   685       0.099        0.072     0.159   \n",
       "17   18      15       14        108   799       0.117        0.202     0.216   \n",
       "18   19      20       14        130  1260       0.098        0.103     0.158   \n",
       "19   20      17       11         87   566       0.098        0.081     0.189   \n",
       "20   21      16       14         86   520       0.108        0.127     0.197   \n",
       "21   22      17       24         60   274       0.102        0.065     0.182   \n",
       "22   23      20       27        103   704       0.107        0.214     0.252   \n",
       "23   24      19       12        137  1404       0.094        0.102     0.177   \n",
       "24   25       9       13        110   905       0.112        0.146     0.200   \n",
       "25   26      19       27        116   913       0.119        0.000     0.304   \n",
       "26   27      10       24         97   645       0.105        0.187     0.225   \n",
       "27   28      16       24        122  1094       0.094        0.107     0.170   \n",
       "28   29      15       15        102   732       0.108        0.170     0.193   \n",
       "29   30      11       16        115   955       0.098        0.116     0.174   \n",
       "..  ...     ...      ...        ...   ...         ...          ...       ...   \n",
       "70   71      21       18        124  1130       0.090        0.103     0.158   \n",
       "71   72       9       26         59   244       0.098        0.153     0.190   \n",
       "72   73      21       12        114   929       0.107        0.183     0.193   \n",
       "73   74      22       25         90   584       0.101        0.128     0.166   \n",
       "74   75      18       13         79   471       0.092        0.068     0.172   \n",
       "75   76      21       18        104   818       0.092        0.084     0.180   \n",
       "76   77      10       17         88   559       0.129        0.105     0.240   \n",
       "77   78      11       21        120  1006       0.107        0.215     0.215   \n",
       "78   79      16       18        144  1245       0.129        0.345     0.291   \n",
       "79   80      22       16         83   506       0.099        0.095     0.172   \n",
       "80   81      10       18         74   402       0.110        0.094     0.184   \n",
       "81   82      17       21         86   520       0.108        0.154     0.194   \n",
       "82   83      10       15        172  1878       0.106        0.267     0.183   \n",
       "83   84      20       14        129  1132       0.122        0.179     0.163   \n",
       "84   85      25       21         77   443       0.097        0.072     0.208   \n",
       "85   86      14       13        121  1075       0.099        0.105     0.213   \n",
       "86   87      19       26         94   648       0.094        0.099     0.208   \n",
       "87   88      19       11        122  1076       0.090        0.121     0.195   \n",
       "88   89      11       11         80   466       0.088        0.094     0.193   \n",
       "89   90      12       23         96   652       0.113        0.134     0.212   \n",
       "90   91      23       27         95   663       0.090        0.086     0.169   \n",
       "91   92      10       12        100   728       0.092        0.104     0.172   \n",
       "92   93      14       14         85   552       0.074        0.051     0.139   \n",
       "93   94      10       17         87   555       0.102        0.082     0.164   \n",
       "94   95      22       26        100   706       0.104        0.155     0.186   \n",
       "95   96      23       16        132  1264       0.091        0.131     0.210   \n",
       "96   97      22       14         78   451       0.105        0.071     0.190   \n",
       "97   98      19       27         62   295       0.102        0.053     0.135   \n",
       "98   99      21       24         74   413       0.090        0.075     0.162   \n",
       "99  100      16       27         94   643       0.098        0.114     0.188   \n",
       "\n",
       "    fractal_dimension diagnosis_result  \n",
       "0               0.079                M  \n",
       "1               0.057                B  \n",
       "2               0.060                M  \n",
       "3               0.097                M  \n",
       "4               0.059                M  \n",
       "5               0.076                B  \n",
       "6               0.057                M  \n",
       "7               0.075                M  \n",
       "8               0.074                M  \n",
       "9               0.082                M  \n",
       "10              0.057                M  \n",
       "11              0.061                M  \n",
       "12              0.078                B  \n",
       "13              0.053                M  \n",
       "14              0.077                M  \n",
       "15              0.071                M  \n",
       "16              0.059                M  \n",
       "17              0.074                M  \n",
       "18              0.000                M  \n",
       "19              0.058                B  \n",
       "20              0.068                B  \n",
       "21              0.069                B  \n",
       "22              0.070                M  \n",
       "23              0.053                M  \n",
       "24              0.063                M  \n",
       "25              0.074                M  \n",
       "26              0.069                M  \n",
       "27              0.057                M  \n",
       "28              0.065                M  \n",
       "29              0.061                M  \n",
       "..                ...              ...  \n",
       "70              0.055                M  \n",
       "71              0.090                B  \n",
       "72              0.065                M  \n",
       "73              0.066                M  \n",
       "74              0.059                B  \n",
       "75              0.054                M  \n",
       "76              0.066                B  \n",
       "77              0.067                M  \n",
       "78              0.081                M  \n",
       "79              0.060                B  \n",
       "80              0.070                B  \n",
       "81              0.069                B  \n",
       "82              0.068                M  \n",
       "83              0.072                M  \n",
       "84              0.060                B  \n",
       "85              0.060                M  \n",
       "86              0.056                M  \n",
       "87              0.056                M  \n",
       "88              0.064                B  \n",
       "89              0.063                B  \n",
       "90              0.059                B  \n",
       "91              0.061                M  \n",
       "92              0.053                B  \n",
       "93              0.057                B  \n",
       "94              0.063                M  \n",
       "95              0.056                M  \n",
       "96              0.066                B  \n",
       "97              0.069                B  \n",
       "98              0.066                B  \n",
       "99              0.064                M  \n",
       "\n",
       "[100 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis_result</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   diagnosis_result  Label\n",
       "0                 M      1\n",
       "1                 B      0\n",
       "2                 M      1\n",
       "3                 M      1\n",
       "4                 M      1\n",
       "5                 B      0\n",
       "6                 M      1\n",
       "7                 M      1\n",
       "8                 M      1\n",
       "9                 M      1\n",
       "10                M      1\n",
       "11                M      1\n",
       "12                B      0\n",
       "13                M      1\n",
       "14                M      1\n",
       "15                M      1\n",
       "16                M      1\n",
       "17                M      1\n",
       "18                M      1\n",
       "19                B      0\n",
       "20                B      0\n",
       "21                B      0\n",
       "22                M      1\n",
       "23                M      1\n",
       "24                M      1\n",
       "25                M      1\n",
       "26                M      1\n",
       "27                M      1\n",
       "28                M      1\n",
       "29                M      1\n",
       "..              ...    ...\n",
       "70                M      1\n",
       "71                B      0\n",
       "72                M      1\n",
       "73                M      1\n",
       "74                B      0\n",
       "75                M      1\n",
       "76                B      0\n",
       "77                M      1\n",
       "78                M      1\n",
       "79                B      0\n",
       "80                B      0\n",
       "81                B      0\n",
       "82                M      1\n",
       "83                M      1\n",
       "84                B      0\n",
       "85                M      1\n",
       "86                M      1\n",
       "87                M      1\n",
       "88                B      0\n",
       "89                B      0\n",
       "90                B      0\n",
       "91                M      1\n",
       "92                B      0\n",
       "93                B      0\n",
       "94                M      1\n",
       "95                M      1\n",
       "96                B      0\n",
       "97                B      0\n",
       "98                B      0\n",
       "99                M      1\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proses\n",
    "\n",
    "\n",
    "##  a.Variance Treshold\n",
    "variance Treshold adalah Selektor fitur yang menghapus semua fitur low-variance.\n",
    "Algoritma pemilihan fitur ini hanya melihat fitur (X), bukan output yang diinginkan (y), dan karenanya dapat digunakan untuk unsupervised learning pada melakukan epoch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.300e+01, 1.200e+01, 1.510e+02, 9.540e+02, 1.430e-01, 2.780e-01,\n",
       "        2.420e-01, 7.900e-02],\n",
       "       [9.000e+00, 1.300e+01, 1.330e+02, 1.326e+03, 1.430e-01, 7.900e-02,\n",
       "        1.810e-01, 5.700e-02],\n",
       "       [2.100e+01, 2.700e+01, 1.300e+02, 1.203e+03, 1.250e-01, 1.600e-01,\n",
       "        2.070e-01, 6.000e-02],\n",
       "       [1.400e+01, 1.600e+01, 7.800e+01, 3.860e+02, 7.000e-02, 2.840e-01,\n",
       "        2.600e-01, 9.700e-02],\n",
       "       [9.000e+00, 1.900e+01, 1.350e+02, 1.297e+03, 1.410e-01, 1.330e-01,\n",
       "        1.810e-01, 5.900e-02],\n",
       "       [2.500e+01, 2.500e+01, 8.300e+01, 4.770e+02, 1.280e-01, 1.700e-01,\n",
       "        2.090e-01, 7.600e-02],\n",
       "       [1.600e+01, 2.600e+01, 1.200e+02, 1.040e+03, 9.500e-02, 1.090e-01,\n",
       "        1.790e-01, 5.700e-02],\n",
       "       [1.500e+01, 1.800e+01, 9.000e+01, 5.780e+02, 1.190e-01, 1.650e-01,\n",
       "        2.200e-01, 7.500e-02],\n",
       "       [1.900e+01, 2.400e+01, 8.800e+01, 5.200e+02, 1.270e-01, 1.930e-01,\n",
       "        2.350e-01, 7.400e-02],\n",
       "       [2.500e+01, 1.100e+01, 8.400e+01, 4.760e+02, 1.190e-01, 2.400e-01,\n",
       "        2.030e-01, 8.200e-02],\n",
       "       [2.400e+01, 2.100e+01, 1.030e+02, 7.980e+02, 8.200e-02, 6.700e-02,\n",
       "        1.530e-01, 5.700e-02],\n",
       "       [1.700e+01, 1.500e+01, 1.040e+02, 7.810e+02, 9.700e-02, 1.290e-01,\n",
       "        1.840e-01, 6.100e-02],\n",
       "       [1.400e+01, 1.500e+01, 1.320e+02, 1.123e+03, 9.700e-02, 0.000e+00,\n",
       "        2.400e-01, 7.800e-02],\n",
       "       [1.200e+01, 2.200e+01, 1.040e+02, 7.830e+02, 8.400e-02, 1.000e-01,\n",
       "        1.850e-01, 5.300e-02],\n",
       "       [1.200e+01, 1.300e+01, 9.400e+01, 5.780e+02, 1.130e-01, 2.290e-01,\n",
       "        2.070e-01, 7.700e-02],\n",
       "       [2.200e+01, 1.900e+01, 9.700e+01, 6.590e+02, 1.140e-01, 1.600e-01,\n",
       "        2.300e-01, 7.100e-02],\n",
       "       [1.000e+01, 1.600e+01, 9.500e+01, 6.850e+02, 9.900e-02, 7.200e-02,\n",
       "        1.590e-01, 5.900e-02],\n",
       "       [1.500e+01, 1.400e+01, 1.080e+02, 7.990e+02, 1.170e-01, 2.020e-01,\n",
       "        2.160e-01, 7.400e-02],\n",
       "       [2.000e+01, 1.400e+01, 1.300e+02, 1.260e+03, 9.800e-02, 1.030e-01,\n",
       "        1.580e-01, 0.000e+00],\n",
       "       [1.700e+01, 1.100e+01, 8.700e+01, 5.660e+02, 9.800e-02, 8.100e-02,\n",
       "        1.890e-01, 5.800e-02],\n",
       "       [1.600e+01, 1.400e+01, 8.600e+01, 5.200e+02, 1.080e-01, 1.270e-01,\n",
       "        1.970e-01, 6.800e-02],\n",
       "       [1.700e+01, 2.400e+01, 6.000e+01, 2.740e+02, 1.020e-01, 6.500e-02,\n",
       "        1.820e-01, 6.900e-02],\n",
       "       [2.000e+01, 2.700e+01, 1.030e+02, 7.040e+02, 1.070e-01, 2.140e-01,\n",
       "        2.520e-01, 7.000e-02],\n",
       "       [1.900e+01, 1.200e+01, 1.370e+02, 1.404e+03, 9.400e-02, 1.020e-01,\n",
       "        1.770e-01, 5.300e-02],\n",
       "       [9.000e+00, 1.300e+01, 1.100e+02, 9.050e+02, 1.120e-01, 1.460e-01,\n",
       "        2.000e-01, 6.300e-02],\n",
       "       [1.900e+01, 2.700e+01, 1.160e+02, 9.130e+02, 1.190e-01, 0.000e+00,\n",
       "        3.040e-01, 7.400e-02],\n",
       "       [1.000e+01, 2.400e+01, 9.700e+01, 6.450e+02, 1.050e-01, 1.870e-01,\n",
       "        2.250e-01, 6.900e-02],\n",
       "       [1.600e+01, 2.400e+01, 1.220e+02, 1.094e+03, 9.400e-02, 1.070e-01,\n",
       "        1.700e-01, 5.700e-02],\n",
       "       [1.500e+01, 1.500e+01, 1.020e+02, 7.320e+02, 1.080e-01, 1.700e-01,\n",
       "        1.930e-01, 6.500e-02],\n",
       "       [1.100e+01, 1.600e+01, 1.150e+02, 9.550e+02, 9.800e-02, 1.160e-01,\n",
       "        1.740e-01, 6.100e-02],\n",
       "       [1.100e+01, 2.200e+01, 1.250e+02, 1.088e+03, 1.060e-01, 1.890e-01,\n",
       "        2.180e-01, 6.200e-02],\n",
       "       [2.300e+01, 2.600e+01, 7.800e+01, 4.410e+02, 1.110e-01, 1.520e-01,\n",
       "        2.300e-01, 7.800e-02],\n",
       "       [2.000e+01, 1.800e+01, 1.130e+02, 8.990e+02, 1.200e-01, 1.500e-01,\n",
       "        2.250e-01, 6.400e-02],\n",
       "       [1.100e+01, 2.100e+01, 1.280e+02, 1.162e+03, 9.400e-02, 1.720e-01,\n",
       "        1.850e-01, 0.000e+00],\n",
       "       [1.600e+01, 2.300e+01, 1.070e+02, 8.070e+02, 1.040e-01, 1.560e-01,\n",
       "        2.000e-01, 6.500e-02],\n",
       "       [1.000e+01, 1.300e+01, 1.100e+02, 8.700e+02, 9.600e-02, 1.340e-01,\n",
       "        1.900e-01, 5.700e-02],\n",
       "       [1.800e+01, 1.200e+01, 9.400e+01, 6.330e+02, 9.800e-02, 0.000e+00,\n",
       "        1.890e-01, 6.100e-02],\n",
       "       [2.100e+01, 1.100e+01, 8.300e+01, 5.240e+02, 9.000e-02, 3.800e-02,\n",
       "        1.470e-01, 5.900e-02],\n",
       "       [1.100e+01, 1.500e+01, 9.600e+01, 6.990e+02, 9.400e-02, 5.100e-02,\n",
       "        1.570e-01, 5.500e-02],\n",
       "       [1.000e+01, 1.400e+01, 8.800e+01, 5.590e+02, 1.020e-01, 1.260e-01,\n",
       "        1.720e-01, 6.400e-02],\n",
       "       [2.400e+01, 1.600e+01, 8.600e+01, 5.630e+02, 8.200e-02, 6.000e-02,\n",
       "        1.780e-01, 5.600e-02],\n",
       "       [1.900e+01, 2.700e+01, 7.200e+01, 3.710e+02, 1.230e-01, 1.220e-01,\n",
       "        1.900e-01, 6.900e-02],\n",
       "       [1.100e+01, 1.100e+01, 1.280e+02, 1.104e+03, 9.100e-02, 2.190e-01,\n",
       "        2.310e-01, 6.300e-02],\n",
       "       [1.500e+01, 2.100e+01, 8.700e+01, 5.450e+02, 1.040e-01, 1.440e-01,\n",
       "        1.970e-01, 6.800e-02],\n",
       "       [1.000e+01, 1.500e+01, 8.500e+01, 5.320e+02, 9.700e-02, 1.050e-01,\n",
       "        1.750e-01, 6.200e-02],\n",
       "       [1.800e+01, 1.100e+01, 1.240e+02, 1.076e+03, 1.100e-01, 1.690e-01,\n",
       "        1.910e-01, 6.000e-02],\n",
       "       [2.200e+01, 1.200e+01, 5.200e+01, 2.020e+02, 8.600e-02, 5.900e-02,\n",
       "        1.770e-01, 6.500e-02],\n",
       "       [2.000e+01, 1.400e+01, 8.600e+01, 5.350e+02, 1.160e-01, 1.230e-01,\n",
       "        2.130e-01, 6.800e-02],\n",
       "       [2.000e+01, 2.100e+01, 7.800e+01, 4.490e+02, 1.030e-01, 9.100e-02,\n",
       "        1.680e-01, 6.000e-02],\n",
       "       [2.500e+01, 1.100e+01, 8.700e+01, 5.610e+02, 8.800e-02, 7.700e-02,\n",
       "        1.810e-01, 5.700e-02],\n",
       "       [1.900e+01, 2.500e+01, 7.500e+01, 4.280e+02, 8.600e-02, 5.000e-02,\n",
       "        1.500e-01, 5.900e-02],\n",
       "       [1.900e+01, 2.200e+01, 8.700e+01, 5.720e+02, 7.700e-02, 6.100e-02,\n",
       "        1.350e-01, 6.000e-02],\n",
       "       [2.500e+01, 1.500e+01, 7.600e+01, 4.380e+02, 8.300e-02, 4.800e-02,\n",
       "        1.870e-01, 6.100e-02],\n",
       "       [1.400e+01, 2.600e+01, 1.200e+02, 1.033e+03, 1.150e-01, 1.490e-01,\n",
       "        2.090e-01, 6.300e-02],\n",
       "       [1.800e+01, 2.500e+01, 9.700e+01, 7.130e+02, 9.100e-02, 7.100e-02,\n",
       "        1.620e-01, 5.700e-02],\n",
       "       [1.800e+01, 1.300e+01, 7.300e+01, 4.090e+02, 9.500e-02, 5.500e-02,\n",
       "        1.920e-01, 5.900e-02],\n",
       "       [1.000e+01, 1.900e+01, 1.260e+02, 1.152e+03, 1.050e-01, 1.270e-01,\n",
       "        1.920e-01, 6.000e-02],\n",
       "       [1.700e+01, 2.000e+01, 9.600e+01, 6.570e+02, 1.140e-01, 1.370e-01,\n",
       "        2.030e-01, 6.800e-02],\n",
       "       [2.200e+01, 1.500e+01, 8.300e+01, 5.270e+02, 8.100e-02, 3.800e-02,\n",
       "        1.820e-01, 5.500e-02],\n",
       "       [2.300e+01, 2.600e+01, 5.400e+01, 2.250e+02, 9.800e-02, 5.300e-02,\n",
       "        1.680e-01, 7.200e-02],\n",
       "       [1.500e+01, 1.800e+01, 6.500e+01, 3.120e+02, 1.130e-01, 8.100e-02,\n",
       "        2.740e-01, 7.000e-02],\n",
       "       [2.500e+01, 1.500e+01, 5.500e+01, 2.220e+02, 1.240e-01, 9.000e-02,\n",
       "        1.830e-01, 6.800e-02],\n",
       "       [1.200e+01, 2.200e+01, 9.600e+01, 6.460e+02, 1.050e-01, 2.010e-01,\n",
       "        1.950e-01, 7.300e-02],\n",
       "       [2.400e+01, 1.700e+01, 5.900e+01, 2.610e+02, 7.700e-02, 8.800e-02,\n",
       "        2.340e-01, 7.000e-02],\n",
       "       [1.600e+01, 1.900e+01, 8.300e+01, 4.990e+02, 1.120e-01, 1.260e-01,\n",
       "        1.910e-01, 6.600e-02],\n",
       "       [1.100e+01, 2.100e+01, 9.700e+01, 6.680e+02, 1.170e-01, 1.480e-01,\n",
       "        1.950e-01, 6.700e-02],\n",
       "       [1.200e+01, 1.300e+01, 6.000e+01, 2.690e+02, 1.040e-01, 7.800e-02,\n",
       "        1.720e-01, 6.900e-02],\n",
       "       [1.800e+01, 1.200e+01, 7.200e+01, 3.940e+02, 8.100e-02, 4.700e-02,\n",
       "        1.520e-01, 5.700e-02],\n",
       "       [1.600e+01, 1.700e+01, 5.900e+01, 2.510e+02, 1.070e-01, 1.410e-01,\n",
       "        2.110e-01, 8.000e-02],\n",
       "       [1.700e+01, 2.100e+01, 8.100e+01, 5.030e+02, 9.800e-02, 5.200e-02,\n",
       "        1.590e-01, 5.700e-02],\n",
       "       [2.100e+01, 1.800e+01, 1.240e+02, 1.130e+03, 9.000e-02, 1.030e-01,\n",
       "        1.580e-01, 5.500e-02],\n",
       "       [9.000e+00, 2.600e+01, 5.900e+01, 2.440e+02, 9.800e-02, 1.530e-01,\n",
       "        1.900e-01, 9.000e-02],\n",
       "       [2.100e+01, 1.200e+01, 1.140e+02, 9.290e+02, 1.070e-01, 1.830e-01,\n",
       "        1.930e-01, 6.500e-02],\n",
       "       [2.200e+01, 2.500e+01, 9.000e+01, 5.840e+02, 1.010e-01, 1.280e-01,\n",
       "        1.660e-01, 6.600e-02],\n",
       "       [1.800e+01, 1.300e+01, 7.900e+01, 4.710e+02, 9.200e-02, 6.800e-02,\n",
       "        1.720e-01, 5.900e-02],\n",
       "       [2.100e+01, 1.800e+01, 1.040e+02, 8.180e+02, 9.200e-02, 8.400e-02,\n",
       "        1.800e-01, 5.400e-02],\n",
       "       [1.000e+01, 1.700e+01, 8.800e+01, 5.590e+02, 1.290e-01, 1.050e-01,\n",
       "        2.400e-01, 6.600e-02],\n",
       "       [1.100e+01, 2.100e+01, 1.200e+02, 1.006e+03, 1.070e-01, 2.150e-01,\n",
       "        2.150e-01, 6.700e-02],\n",
       "       [1.600e+01, 1.800e+01, 1.440e+02, 1.245e+03, 1.290e-01, 3.450e-01,\n",
       "        2.910e-01, 8.100e-02],\n",
       "       [2.200e+01, 1.600e+01, 8.300e+01, 5.060e+02, 9.900e-02, 9.500e-02,\n",
       "        1.720e-01, 6.000e-02],\n",
       "       [1.000e+01, 1.800e+01, 7.400e+01, 4.020e+02, 1.100e-01, 9.400e-02,\n",
       "        1.840e-01, 7.000e-02],\n",
       "       [1.700e+01, 2.100e+01, 8.600e+01, 5.200e+02, 1.080e-01, 1.540e-01,\n",
       "        1.940e-01, 6.900e-02],\n",
       "       [1.000e+01, 1.500e+01, 1.720e+02, 1.878e+03, 1.060e-01, 2.670e-01,\n",
       "        1.830e-01, 6.800e-02],\n",
       "       [2.000e+01, 1.400e+01, 1.290e+02, 1.132e+03, 1.220e-01, 1.790e-01,\n",
       "        1.630e-01, 7.200e-02],\n",
       "       [2.500e+01, 2.100e+01, 7.700e+01, 4.430e+02, 9.700e-02, 7.200e-02,\n",
       "        2.080e-01, 6.000e-02],\n",
       "       [1.400e+01, 1.300e+01, 1.210e+02, 1.075e+03, 9.900e-02, 1.050e-01,\n",
       "        2.130e-01, 6.000e-02],\n",
       "       [1.900e+01, 2.600e+01, 9.400e+01, 6.480e+02, 9.400e-02, 9.900e-02,\n",
       "        2.080e-01, 5.600e-02],\n",
       "       [1.900e+01, 1.100e+01, 1.220e+02, 1.076e+03, 9.000e-02, 1.210e-01,\n",
       "        1.950e-01, 5.600e-02],\n",
       "       [1.100e+01, 1.100e+01, 8.000e+01, 4.660e+02, 8.800e-02, 9.400e-02,\n",
       "        1.930e-01, 6.400e-02],\n",
       "       [1.200e+01, 2.300e+01, 9.600e+01, 6.520e+02, 1.130e-01, 1.340e-01,\n",
       "        2.120e-01, 6.300e-02],\n",
       "       [2.300e+01, 2.700e+01, 9.500e+01, 6.630e+02, 9.000e-02, 8.600e-02,\n",
       "        1.690e-01, 5.900e-02],\n",
       "       [1.000e+01, 1.200e+01, 1.000e+02, 7.280e+02, 9.200e-02, 1.040e-01,\n",
       "        1.720e-01, 6.100e-02],\n",
       "       [1.400e+01, 1.400e+01, 8.500e+01, 5.520e+02, 7.400e-02, 5.100e-02,\n",
       "        1.390e-01, 5.300e-02],\n",
       "       [1.000e+01, 1.700e+01, 8.700e+01, 5.550e+02, 1.020e-01, 8.200e-02,\n",
       "        1.640e-01, 5.700e-02],\n",
       "       [2.200e+01, 2.600e+01, 1.000e+02, 7.060e+02, 1.040e-01, 1.550e-01,\n",
       "        1.860e-01, 6.300e-02],\n",
       "       [2.300e+01, 1.600e+01, 1.320e+02, 1.264e+03, 9.100e-02, 1.310e-01,\n",
       "        2.100e-01, 5.600e-02],\n",
       "       [2.200e+01, 1.400e+01, 7.800e+01, 4.510e+02, 1.050e-01, 7.100e-02,\n",
       "        1.900e-01, 6.600e-02],\n",
       "       [1.900e+01, 2.700e+01, 6.200e+01, 2.950e+02, 1.020e-01, 5.300e-02,\n",
       "        1.350e-01, 6.900e-02],\n",
       "       [2.100e+01, 2.400e+01, 7.400e+01, 4.130e+02, 9.000e-02, 7.500e-02,\n",
       "        1.620e-01, 6.600e-02],\n",
       "       [1.600e+01, 2.700e+01, 9.400e+01, 6.430e+02, 9.800e-02, 1.140e-01,\n",
       "        1.880e-01, 6.400e-02]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y\n",
    "lab_0 = np.where(Y == 0)[0]\n",
    "lab_1 = np.where(Y == 1)[0]\n",
    "# print((len(lab_0), len(lab_1)))\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "model_selection = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "new_feature = model_selection.fit_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Perubahan data terjadi ketika mengalami proses Variance Treshold, yang dimana data sebelum melakukan proses variance treshold memiliki 8 input dan setelah melalui proses variance Treshold data hanyak memiliki 4 input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Sebelum Variance Treshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Sesudah Variance Treshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_feature.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Preprocessing\n",
    "\n",
    "Data preprocessing dilakukan untuk menghilangkan noise pada detaset, preprocessing meliputi pembersihan, pemilihan Instance, normalisasi, transformasi, ekstraksi fitur dan seleksi.\n",
    "    \n",
    "Preprocessing menggunakan StandardScaler. StandardScaler adalah fitur  Standarisasi  dengan menghapus mean dan scaling ke varians satuan\n",
    "Skor standar dari suatu sampel x dihitung sebagai:\n",
    "z = (x - u) / s\n",
    "di mana u adalah mean dari sampel data training atau nol jika with_mean = False, dan s adalah standar deviasi dari sampel pelatihan atau satu jika with_std = False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = scaler.fit_transform(new_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.26683005e+00, -1.20574648e+00,  2.30161118e+00,\n",
       "         7.89416627e-01],\n",
       "       [-1.61701072e+00, -1.01220772e+00,  1.53752042e+00,\n",
       "         1.95882960e+00],\n",
       "       [ 8.54852801e-01,  1.69733493e+00,  1.41017196e+00,\n",
       "         1.57216886e+00],\n",
       "       [-5.87067586e-01, -4.31591436e-01, -7.97201366e-01,\n",
       "        -9.96138662e-01],\n",
       "       [-1.61701072e+00,  1.49024846e-01,  1.62241939e+00,\n",
       "         1.86766568e+00],\n",
       "       [ 1.67880731e+00,  1.31025741e+00, -5.84953931e-01,\n",
       "        -7.10072586e-01],\n",
       "       [-1.75090333e-01,  1.50379617e+00,  9.85677088e-01,\n",
       "         1.05976479e+00],\n",
       "       [-3.81078959e-01, -4.45139150e-02, -2.87807522e-01,\n",
       "        -3.92570677e-01],\n",
       "       [ 4.42875547e-01,  1.11671865e+00, -3.72706496e-01,\n",
       "        -5.74898506e-01],\n",
       "       [ 1.67880731e+00, -1.39928524e+00, -5.42504444e-01,\n",
       "        -7.13216169e-01],\n",
       "       [ 1.47281868e+00,  5.36102367e-01,  2.64035809e-01,\n",
       "         2.99017639e-01],\n",
       "       [ 3.08982940e-02, -6.25130197e-01,  3.06485296e-01,\n",
       "         2.45576724e-01],\n",
       "       [-5.87067586e-01, -6.25130197e-01,  1.49507093e+00,\n",
       "         1.32068220e+00],\n",
       "       [-9.99044839e-01,  7.29641128e-01,  3.06485296e-01,\n",
       "         2.51863890e-01],\n",
       "       [-9.99044839e-01, -1.01220772e+00, -1.18009574e-01,\n",
       "        -3.92570677e-01],\n",
       "       [ 1.06084143e+00,  1.49024846e-01,  9.33888714e-03,\n",
       "        -1.37940433e-01],\n",
       "       [-1.41102209e+00, -4.31591436e-01, -7.55600868e-02,\n",
       "        -5.62072686e-02],\n",
       "       [-3.81078959e-01, -8.18668958e-01,  4.76283244e-01,\n",
       "         3.02161223e-01],\n",
       "       [ 6.48864174e-01, -8.18668958e-01,  1.41017196e+00,\n",
       "         1.75135310e+00],\n",
       "       [ 3.08982940e-02, -1.39928524e+00, -4.15155983e-01,\n",
       "        -4.30293676e-01],\n",
       "       [-1.75090333e-01, -8.18668958e-01, -4.57605470e-01,\n",
       "        -5.74898506e-01],\n",
       "       [ 3.08982940e-02,  1.11671865e+00, -1.56129213e+00,\n",
       "        -1.34821999e+00],\n",
       "       [ 6.48864174e-01,  1.69733493e+00,  2.64035809e-01,\n",
       "         3.52081325e-03],\n",
       "       [ 4.42875547e-01, -1.20574648e+00,  1.70731837e+00,\n",
       "         2.20402909e+00],\n",
       "       [-1.61701072e+00, -1.01220772e+00,  5.61182218e-01,\n",
       "         6.35381048e-01],\n",
       "       [ 4.42875547e-01,  1.69733493e+00,  8.15879140e-01,\n",
       "         6.60529714e-01],\n",
       "       [-1.41102209e+00,  1.11671865e+00,  9.33888714e-03,\n",
       "        -1.81950599e-01],\n",
       "       [-1.75090333e-01,  1.11671865e+00,  1.07057606e+00,\n",
       "         1.22951828e+00],\n",
       "       [-3.81078959e-01, -6.25130197e-01,  2.21586322e-01,\n",
       "         9.15411444e-02],\n",
       "       [-1.20503347e+00, -4.31591436e-01,  7.73429653e-01,\n",
       "         7.92560211e-01],\n",
       "       [-1.20503347e+00,  7.29641128e-01,  1.19792452e+00,\n",
       "         1.21065678e+00],\n",
       "       [ 1.26683005e+00,  1.50379617e+00, -7.97201366e-01,\n",
       "        -8.23241583e-01],\n",
       "       [ 6.48864174e-01, -4.45139150e-02,  6.88530679e-01,\n",
       "         6.16519548e-01],\n",
       "       [-1.20503347e+00,  5.36102367e-01,  1.32527298e+00,\n",
       "         1.44328194e+00],\n",
       "       [-1.75090333e-01,  9.23179889e-01,  4.33833757e-01,\n",
       "         3.27309889e-01],\n",
       "       [-1.41102209e+00, -1.01220772e+00,  5.61182218e-01,\n",
       "         5.25355634e-01],\n",
       "       [ 2.36886921e-01, -1.20574648e+00, -1.18009574e-01,\n",
       "        -2.19673598e-01],\n",
       "       [ 8.54852801e-01, -1.39928524e+00, -5.84953931e-01,\n",
       "        -5.62324173e-01],\n",
       "       [-1.20503347e+00, -6.25130197e-01, -3.31105999e-02,\n",
       "        -1.21971030e-02],\n",
       "       [-1.41102209e+00, -8.18668958e-01, -3.72706496e-01,\n",
       "        -4.52298759e-01],\n",
       "       [ 1.47281868e+00, -4.31591436e-01, -4.57605470e-01,\n",
       "        -4.39724426e-01],\n",
       "       [ 4.42875547e-01,  1.69733493e+00, -1.05189829e+00,\n",
       "        -1.04329241e+00],\n",
       "       [-1.20503347e+00, -1.39928524e+00,  1.32527298e+00,\n",
       "         1.26095412e+00],\n",
       "       [-3.81078959e-01,  5.36102367e-01, -4.15155983e-01,\n",
       "        -4.96308924e-01],\n",
       "       [-1.41102209e+00, -6.25130197e-01, -5.00054957e-01,\n",
       "        -5.37175507e-01],\n",
       "       [ 2.36886921e-01, -1.39928524e+00,  1.15547504e+00,\n",
       "         1.17293378e+00],\n",
       "       [ 1.06084143e+00, -1.20574648e+00, -1.90088803e+00,\n",
       "        -1.57455798e+00],\n",
       "       [ 6.48864174e-01, -8.18668958e-01, -4.57605470e-01,\n",
       "        -5.27744757e-01],\n",
       "       [ 6.48864174e-01,  5.36102367e-01, -7.97201366e-01,\n",
       "        -7.98092917e-01],\n",
       "       [ 1.67880731e+00, -1.39928524e+00, -4.15155983e-01,\n",
       "        -4.46011592e-01],\n",
       "       [ 4.42875547e-01,  1.31025741e+00, -9.24549827e-01,\n",
       "        -8.64108165e-01],\n",
       "       [ 4.42875547e-01,  7.29641128e-01, -4.15155983e-01,\n",
       "        -4.11432177e-01],\n",
       "       [ 1.67880731e+00, -6.25130197e-01, -8.82100340e-01,\n",
       "        -8.32672333e-01],\n",
       "       [-5.87067586e-01,  1.50379617e+00,  9.85677088e-01,\n",
       "         1.03775970e+00],\n",
       "       [ 2.36886921e-01,  1.31025741e+00,  9.33888714e-03,\n",
       "         3.18130626e-02],\n",
       "       [ 2.36886921e-01, -1.01220772e+00, -1.00944880e+00,\n",
       "        -9.23836247e-01],\n",
       "       [-1.41102209e+00,  1.49024846e-01,  1.24037401e+00,\n",
       "         1.41184611e+00],\n",
       "       [ 3.08982940e-02,  3.42563607e-01, -3.31105999e-02,\n",
       "        -1.44227600e-01],\n",
       "       [ 1.06084143e+00, -6.25130197e-01, -5.84953931e-01,\n",
       "        -5.52893423e-01],\n",
       "       [ 1.26683005e+00,  1.50379617e+00, -1.81598905e+00,\n",
       "        -1.50225557e+00],\n",
       "       [-3.81078959e-01, -4.45139150e-02, -1.34904470e+00,\n",
       "        -1.22876382e+00],\n",
       "       [ 1.67880731e+00, -6.25130197e-01, -1.77353957e+00,\n",
       "        -1.51168632e+00],\n",
       "       [-9.99044839e-01,  7.29641128e-01, -3.31105999e-02,\n",
       "        -1.78807016e-01],\n",
       "       [ 1.47281868e+00, -2.38052676e-01, -1.60374162e+00,\n",
       "        -1.38908657e+00],\n",
       "       [-1.75090333e-01,  1.49024846e-01, -5.84953931e-01,\n",
       "        -6.40913754e-01],\n",
       "       [-1.20503347e+00,  5.36102367e-01,  9.33888714e-03,\n",
       "        -1.09648184e-01],\n",
       "       [-9.99044839e-01, -1.01220772e+00, -1.56129213e+00,\n",
       "        -1.36393790e+00],\n",
       "       [ 2.36886921e-01, -1.20574648e+00, -1.05189829e+00,\n",
       "        -9.70989996e-01],\n",
       "       [-1.75090333e-01, -2.38052676e-01, -1.60374162e+00,\n",
       "        -1.42052240e+00],\n",
       "       [ 3.08982940e-02,  5.36102367e-01, -6.69852905e-01,\n",
       "        -6.28339421e-01],\n",
       "       [ 8.54852801e-01, -4.45139150e-02,  1.15547504e+00,\n",
       "         1.34268728e+00],\n",
       "       [-1.61701072e+00,  1.50379617e+00, -1.60374162e+00,\n",
       "        -1.44252748e+00],\n",
       "       [ 8.54852801e-01, -1.20574648e+00,  7.30980166e-01,\n",
       "         7.10827046e-01],\n",
       "       [ 1.06084143e+00,  1.31025741e+00, -2.87807522e-01,\n",
       "        -3.73709177e-01],\n",
       "       [ 2.36886921e-01, -1.01220772e+00, -7.54751879e-01,\n",
       "        -7.28934085e-01],\n",
       "       [ 8.54852801e-01, -4.45139150e-02,  3.06485296e-01,\n",
       "         3.61889304e-01],\n",
       "       [-1.41102209e+00, -2.38052676e-01, -3.72706496e-01,\n",
       "        -4.52298759e-01],\n",
       "       [-1.20503347e+00,  5.36102367e-01,  9.85677088e-01,\n",
       "         9.52882957e-01],\n",
       "       [-1.75090333e-01, -4.45139150e-02,  2.00446478e+00,\n",
       "         1.70419935e+00],\n",
       "       [ 1.06084143e+00, -4.31591436e-01, -5.84953931e-01,\n",
       "        -6.18908671e-01],\n",
       "       [-1.41102209e+00, -4.45139150e-02, -9.66999314e-01,\n",
       "        -9.45841330e-01],\n",
       "       [ 3.08982940e-02,  5.36102367e-01, -4.57605470e-01,\n",
       "        -5.74898506e-01],\n",
       "       [-1.41102209e+00, -6.25130197e-01,  3.19305041e+00,\n",
       "         3.69408756e+00],\n",
       "       [ 6.48864174e-01, -8.18668958e-01,  1.36772247e+00,\n",
       "         1.34897445e+00],\n",
       "       [ 1.67880731e+00,  5.36102367e-01, -8.39650853e-01,\n",
       "        -8.16954417e-01],\n",
       "       [-5.87067586e-01, -1.01220772e+00,  1.02812657e+00,\n",
       "         1.16979020e+00],\n",
       "       [ 4.42875547e-01,  1.50379617e+00, -1.18009574e-01,\n",
       "        -1.72519849e-01],\n",
       "       [ 4.42875547e-01, -1.39928524e+00,  1.07057606e+00,\n",
       "         1.17293378e+00],\n",
       "       [-1.20503347e+00, -1.39928524e+00, -7.12302392e-01,\n",
       "        -7.44652002e-01],\n",
       "       [-9.99044839e-01,  9.23179889e-01, -3.31105999e-02,\n",
       "        -1.59945516e-01],\n",
       "       [ 1.26683005e+00,  1.69733493e+00, -7.55600868e-02,\n",
       "        -1.25366100e-01],\n",
       "       [-1.41102209e+00, -1.20574648e+00,  1.36687348e-01,\n",
       "         7.89668114e-02],\n",
       "       [-5.87067586e-01, -8.18668958e-01, -5.00054957e-01,\n",
       "        -4.74303842e-01],\n",
       "       [-1.41102209e+00, -2.38052676e-01, -4.15155983e-01,\n",
       "        -4.64873092e-01],\n",
       "       [ 1.06084143e+00,  1.50379617e+00,  1.36687348e-01,\n",
       "         9.80797976e-03],\n",
       "       [ 1.26683005e+00, -4.31591436e-01,  1.49507093e+00,\n",
       "         1.76392744e+00],\n",
       "       [ 1.06084143e+00, -8.18668958e-01, -7.97201366e-01,\n",
       "        -7.91805751e-01],\n",
       "       [ 4.42875547e-01,  1.69733493e+00, -1.47639316e+00,\n",
       "        -1.28220474e+00],\n",
       "       [ 8.54852801e-01,  1.11671865e+00, -9.66999314e-01,\n",
       "        -9.11261914e-01],\n",
       "       [-1.75090333e-01,  1.69733493e+00, -1.18009574e-01,\n",
       "        -1.88237765e-01]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label\n",
    "\n",
    "Memberikan Variable pada label agar bisa digunakan untuk perhitungan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "       1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
       "       1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mendefinisikan Model \n",
    "\n",
    "## a. Split Data\n",
    "\n",
    "Pada proses Neural Network data harus dibagi menjadi 2 yaitu\n",
    "\n",
    "    -Data Test\n",
    "    -Data Training\n",
    "\n",
    "Data training adalah data yang digunakan untuk melakukan learning\n",
    "\n",
    "Data test adalah data validasi yang digunakan untuk melakukan evaluasi terhadap performa yang telah dilakukan learning pada data training\n",
    "\n",
    "   data feature dan label akan dibagi sesuai yang diinginkan, disini pembagian data sebanyak 20% untuk data tes dari total dataset. total dataset yang digunakan sebanyak 100 data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_Y, test_Y = train_test_split(X_scaled,Y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Model\n",
    "\n",
    "pada learning data training, proses menggunakan library model dan layers dari keras\n",
    "\n",
    "Mendefinisikan model sequential() yang ditampung kedalam variabel model\n",
    "\n",
    "## Input layer\n",
    "input memiliki 1 layer yang menggunakan aktivasi relu dengan nilai neuron sebanyak 12 neuro dan data input 4\n",
    "\n",
    "## Hidden Layer\n",
    " \n",
    "hidden layer yang digunakan sebanyak 1 hidden layers dengan menggunakan aktivasi Relu dengan neuron sebanyak 12 neuron yang terhubung juga dengan input layers yang memiliki 12 node juga. penggunaan 1 hidden layer digunakan karena penyesuaian dari data feature dan pembagian data tes dan data training.\n",
    "    \n",
    "## Output Layer\n",
    "output layers memiliki 1 layers dengan 1 node dengan menggunakan aktivasi sigmoid. Fungsi sigmoid akan menerima angka tunggal pada output dan merubah nilai X menjadi nilai yang memiliki range 0 sampai 1.\n",
    "\n",
    "##  Model.fit\n",
    "pada model fit menggunakan optimizer adam dan loss function binary_crossentropy. Pemakaian loss function tersebut karena label output hanya memiliki 2 tipe 0 dan 1 yang dimana data tersebut adalah biner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/300\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.7497 - acc: 0.3000 - val_loss: 0.7942 - val_acc: 0.3000\n",
      "Epoch 2/300\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.7341 - acc: 0.3625 - val_loss: 0.7772 - val_acc: 0.3000\n",
      "Epoch 3/300\n",
      "80/80 [==============================] - 0s 74us/step - loss: 0.7196 - acc: 0.3875 - val_loss: 0.7602 - val_acc: 0.3000\n",
      "Epoch 4/300\n",
      "80/80 [==============================] - 0s 88us/step - loss: 0.7052 - acc: 0.4625 - val_loss: 0.7435 - val_acc: 0.3500\n",
      "Epoch 5/300\n",
      "80/80 [==============================] - 0s 88us/step - loss: 0.6923 - acc: 0.4875 - val_loss: 0.7267 - val_acc: 0.3500\n",
      "Epoch 6/300\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.6785 - acc: 0.5375 - val_loss: 0.7109 - val_acc: 0.4000\n",
      "Epoch 7/300\n",
      "80/80 [==============================] - 0s 88us/step - loss: 0.6652 - acc: 0.6000 - val_loss: 0.6963 - val_acc: 0.4500\n",
      "Epoch 8/300\n",
      "80/80 [==============================] - 0s 89us/step - loss: 0.6528 - acc: 0.6500 - val_loss: 0.6826 - val_acc: 0.4500\n",
      "Epoch 9/300\n",
      "80/80 [==============================] - 0s 88us/step - loss: 0.6423 - acc: 0.6625 - val_loss: 0.6690 - val_acc: 0.5000\n",
      "Epoch 10/300\n",
      "80/80 [==============================] - 0s 73us/step - loss: 0.6306 - acc: 0.6750 - val_loss: 0.6560 - val_acc: 0.5000\n",
      "Epoch 11/300\n",
      "80/80 [==============================] - 0s 68us/step - loss: 0.6194 - acc: 0.6875 - val_loss: 0.6439 - val_acc: 0.5500\n",
      "Epoch 12/300\n",
      "80/80 [==============================] - 0s 67us/step - loss: 0.6094 - acc: 0.7250 - val_loss: 0.6313 - val_acc: 0.5500\n",
      "Epoch 13/300\n",
      "80/80 [==============================] - 0s 77us/step - loss: 0.5987 - acc: 0.7500 - val_loss: 0.6188 - val_acc: 0.6500\n",
      "Epoch 14/300\n",
      "80/80 [==============================] - 0s 113us/step - loss: 0.5875 - acc: 0.7500 - val_loss: 0.6063 - val_acc: 0.8000\n",
      "Epoch 15/300\n",
      "80/80 [==============================] - 0s 84us/step - loss: 0.5779 - acc: 0.8000 - val_loss: 0.5937 - val_acc: 0.8000\n",
      "Epoch 16/300\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.5678 - acc: 0.8000 - val_loss: 0.5812 - val_acc: 0.8000\n",
      "Epoch 17/300\n",
      "80/80 [==============================] - 0s 76us/step - loss: 0.5590 - acc: 0.8000 - val_loss: 0.5690 - val_acc: 0.7500\n",
      "Epoch 18/300\n",
      "80/80 [==============================] - 0s 74us/step - loss: 0.5500 - acc: 0.8250 - val_loss: 0.5569 - val_acc: 0.7500\n",
      "Epoch 19/300\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.5402 - acc: 0.8250 - val_loss: 0.5456 - val_acc: 0.8000\n",
      "Epoch 20/300\n",
      "80/80 [==============================] - 0s 98us/step - loss: 0.5347 - acc: 0.8250 - val_loss: 0.5332 - val_acc: 0.8000\n",
      "Epoch 21/300\n",
      "80/80 [==============================] - 0s 76us/step - loss: 0.5267 - acc: 0.8250 - val_loss: 0.5220 - val_acc: 0.8000\n",
      "Epoch 22/300\n",
      "80/80 [==============================] - 0s 73us/step - loss: 0.5198 - acc: 0.8250 - val_loss: 0.5110 - val_acc: 0.8000\n",
      "Epoch 23/300\n",
      "80/80 [==============================] - 0s 77us/step - loss: 0.5133 - acc: 0.8125 - val_loss: 0.5013 - val_acc: 0.8000\n",
      "Epoch 24/300\n",
      "80/80 [==============================] - 0s 76us/step - loss: 0.5077 - acc: 0.8125 - val_loss: 0.4918 - val_acc: 0.8000\n",
      "Epoch 25/300\n",
      "80/80 [==============================] - 0s 73us/step - loss: 0.5023 - acc: 0.8000 - val_loss: 0.4815 - val_acc: 0.8000\n",
      "Epoch 26/300\n",
      "80/80 [==============================] - 0s 74us/step - loss: 0.4956 - acc: 0.8000 - val_loss: 0.4721 - val_acc: 0.8000\n",
      "Epoch 27/300\n",
      "80/80 [==============================] - 0s 77us/step - loss: 0.4911 - acc: 0.8000 - val_loss: 0.4626 - val_acc: 0.8000\n",
      "Epoch 28/300\n",
      "80/80 [==============================] - 0s 72us/step - loss: 0.4856 - acc: 0.8000 - val_loss: 0.4539 - val_acc: 0.8000\n",
      "Epoch 29/300\n",
      "80/80 [==============================] - 0s 66us/step - loss: 0.4811 - acc: 0.8000 - val_loss: 0.4456 - val_acc: 0.8000\n",
      "Epoch 30/300\n",
      "80/80 [==============================] - 0s 74us/step - loss: 0.4766 - acc: 0.8000 - val_loss: 0.4370 - val_acc: 0.8000\n",
      "Epoch 31/300\n",
      "80/80 [==============================] - 0s 76us/step - loss: 0.4715 - acc: 0.8000 - val_loss: 0.4296 - val_acc: 0.8000\n",
      "Epoch 32/300\n",
      "80/80 [==============================] - 0s 73us/step - loss: 0.4678 - acc: 0.8000 - val_loss: 0.4219 - val_acc: 0.8000\n",
      "Epoch 33/300\n",
      "80/80 [==============================] - 0s 74us/step - loss: 0.4640 - acc: 0.7875 - val_loss: 0.4152 - val_acc: 0.8000\n",
      "Epoch 34/300\n",
      "80/80 [==============================] - 0s 89us/step - loss: 0.4607 - acc: 0.7875 - val_loss: 0.4085 - val_acc: 0.8000\n",
      "Epoch 35/300\n",
      "80/80 [==============================] - 0s 72us/step - loss: 0.4583 - acc: 0.8000 - val_loss: 0.4013 - val_acc: 0.8000\n",
      "Epoch 36/300\n",
      "80/80 [==============================] - 0s 125us/step - loss: 0.4539 - acc: 0.8000 - val_loss: 0.3957 - val_acc: 0.8500\n",
      "Epoch 37/300\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.4518 - acc: 0.8000 - val_loss: 0.3894 - val_acc: 0.8500\n",
      "Epoch 38/300\n",
      "80/80 [==============================] - 0s 131us/step - loss: 0.4488 - acc: 0.8000 - val_loss: 0.3842 - val_acc: 0.8500\n",
      "Epoch 39/300\n",
      "80/80 [==============================] - 0s 121us/step - loss: 0.4467 - acc: 0.8000 - val_loss: 0.3787 - val_acc: 0.8500\n",
      "Epoch 40/300\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.4443 - acc: 0.8000 - val_loss: 0.3740 - val_acc: 0.8500\n",
      "Epoch 41/300\n",
      "80/80 [==============================] - 0s 89us/step - loss: 0.4418 - acc: 0.8000 - val_loss: 0.3705 - val_acc: 0.8500\n",
      "Epoch 42/300\n",
      "80/80 [==============================] - 0s 86us/step - loss: 0.4397 - acc: 0.8000 - val_loss: 0.3668 - val_acc: 0.8500\n",
      "Epoch 43/300\n",
      "80/80 [==============================] - 0s 78us/step - loss: 0.4379 - acc: 0.8000 - val_loss: 0.3633 - val_acc: 0.8500\n",
      "Epoch 44/300\n",
      "80/80 [==============================] - 0s 86us/step - loss: 0.4363 - acc: 0.8000 - val_loss: 0.3601 - val_acc: 0.8500\n",
      "Epoch 45/300\n",
      "80/80 [==============================] - 0s 74us/step - loss: 0.4346 - acc: 0.8000 - val_loss: 0.3566 - val_acc: 0.8500\n",
      "Epoch 46/300\n",
      "80/80 [==============================] - 0s 86us/step - loss: 0.4332 - acc: 0.8000 - val_loss: 0.3535 - val_acc: 0.8500\n",
      "Epoch 47/300\n",
      "80/80 [==============================] - 0s 76us/step - loss: 0.4320 - acc: 0.8000 - val_loss: 0.3514 - val_acc: 0.8500\n",
      "Epoch 48/300\n",
      "80/80 [==============================] - 0s 72us/step - loss: 0.4304 - acc: 0.8000 - val_loss: 0.3486 - val_acc: 0.8500\n",
      "Epoch 49/300\n",
      "80/80 [==============================] - 0s 0us/step - loss: 0.4294 - acc: 0.8000 - val_loss: 0.3459 - val_acc: 0.8500\n",
      "Epoch 50/300\n",
      "80/80 [==============================] - 0s 181us/step - loss: 0.4278 - acc: 0.8000 - val_loss: 0.3437 - val_acc: 0.8500\n",
      "Epoch 51/300\n",
      "80/80 [==============================] - 0s 110us/step - loss: 0.4269 - acc: 0.8000 - val_loss: 0.3418 - val_acc: 0.8500\n",
      "Epoch 52/300\n",
      "80/80 [==============================] - 0s 13us/step - loss: 0.4263 - acc: 0.8000 - val_loss: 0.3395 - val_acc: 0.8500\n",
      "Epoch 53/300\n",
      "80/80 [==============================] - 0s 148us/step - loss: 0.4246 - acc: 0.8000 - val_loss: 0.3375 - val_acc: 0.8500\n",
      "Epoch 54/300\n",
      "80/80 [==============================] - 0s 101us/step - loss: 0.4240 - acc: 0.8000 - val_loss: 0.3362 - val_acc: 0.8500\n",
      "Epoch 55/300\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.4229 - acc: 0.8000 - val_loss: 0.3344 - val_acc: 0.8500\n",
      "Epoch 56/300\n",
      "80/80 [==============================] - 0s 88us/step - loss: 0.4220 - acc: 0.8000 - val_loss: 0.3331 - val_acc: 0.8500\n",
      "Epoch 57/300\n",
      "80/80 [==============================] - 0s 90us/step - loss: 0.4211 - acc: 0.8000 - val_loss: 0.3317 - val_acc: 0.8500\n",
      "Epoch 58/300\n",
      "80/80 [==============================] - 0s 99us/step - loss: 0.4205 - acc: 0.8000 - val_loss: 0.3314 - val_acc: 0.8500\n",
      "Epoch 59/300\n",
      "80/80 [==============================] - 0s 86us/step - loss: 0.4195 - acc: 0.8000 - val_loss: 0.3297 - val_acc: 0.8500\n",
      "Epoch 60/300\n",
      "80/80 [==============================] - 0s 83us/step - loss: 0.4188 - acc: 0.8000 - val_loss: 0.3287 - val_acc: 0.8500\n",
      "Epoch 61/300\n",
      "80/80 [==============================] - 0s 91us/step - loss: 0.4179 - acc: 0.8000 - val_loss: 0.3272 - val_acc: 0.8500\n",
      "Epoch 62/300\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.4170 - acc: 0.8000 - val_loss: 0.3256 - val_acc: 0.8500\n",
      "Epoch 63/300\n",
      "80/80 [==============================] - 0s 77us/step - loss: 0.4162 - acc: 0.8000 - val_loss: 0.3249 - val_acc: 0.8500\n",
      "Epoch 64/300\n",
      "80/80 [==============================] - 0s 97us/step - loss: 0.4154 - acc: 0.8000 - val_loss: 0.3236 - val_acc: 0.8500\n",
      "Epoch 65/300\n",
      "80/80 [==============================] - 0s 118us/step - loss: 0.4147 - acc: 0.8000 - val_loss: 0.3231 - val_acc: 0.8500\n",
      "Epoch 66/300\n",
      "80/80 [==============================] - 0s 110us/step - loss: 0.4138 - acc: 0.8000 - val_loss: 0.3218 - val_acc: 0.8500\n",
      "Epoch 67/300\n",
      "80/80 [==============================] - 0s 102us/step - loss: 0.4135 - acc: 0.8000 - val_loss: 0.3213 - val_acc: 0.8500\n",
      "Epoch 68/300\n",
      "80/80 [==============================] - 0s 47us/step - loss: 0.4123 - acc: 0.8125 - val_loss: 0.3206 - val_acc: 0.8500\n",
      "Epoch 69/300\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.4124 - acc: 0.8125 - val_loss: 0.3192 - val_acc: 0.8500\n",
      "Epoch 70/300\n",
      "80/80 [==============================] - 0s 0us/step - loss: 0.4110 - acc: 0.8125 - val_loss: 0.3184 - val_acc: 0.8500\n",
      "Epoch 71/300\n",
      "80/80 [==============================] - 0s 0us/step - loss: 0.4105 - acc: 0.8125 - val_loss: 0.3176 - val_acc: 0.8500\n",
      "Epoch 72/300\n",
      "80/80 [==============================] - 0s 270us/step - loss: 0.4097 - acc: 0.8125 - val_loss: 0.3170 - val_acc: 0.8500\n",
      "Epoch 73/300\n",
      "80/80 [==============================] - 0s 0us/step - loss: 0.4092 - acc: 0.8000 - val_loss: 0.3163 - val_acc: 0.8500\n",
      "Epoch 74/300\n",
      "80/80 [==============================] - 0s 0us/step - loss: 0.4087 - acc: 0.8000 - val_loss: 0.3167 - val_acc: 0.8500\n",
      "Epoch 75/300\n",
      "80/80 [==============================] - 0s 273us/step - loss: 0.4078 - acc: 0.8125 - val_loss: 0.3158 - val_acc: 0.8500\n",
      "Epoch 76/300\n",
      "80/80 [==============================] - 0s 89us/step - loss: 0.4075 - acc: 0.8125 - val_loss: 0.3158 - val_acc: 0.8500\n",
      "Epoch 77/300\n",
      "80/80 [==============================] - 0s 93us/step - loss: 0.4068 - acc: 0.8125 - val_loss: 0.3146 - val_acc: 0.8500\n",
      "Epoch 78/300\n",
      "80/80 [==============================] - 0s 12us/step - loss: 0.4061 - acc: 0.8125 - val_loss: 0.3145 - val_acc: 0.8500\n",
      "Epoch 79/300\n",
      "80/80 [==============================] - 0s 0us/step - loss: 0.4057 - acc: 0.8125 - val_loss: 0.3143 - val_acc: 0.8500\n",
      "Epoch 80/300\n",
      "80/80 [==============================] - 0s 210us/step - loss: 0.4051 - acc: 0.8125 - val_loss: 0.3133 - val_acc: 0.8500\n",
      "Epoch 81/300\n",
      "80/80 [==============================] - 0s 90us/step - loss: 0.4046 - acc: 0.8125 - val_loss: 0.3133 - val_acc: 0.8500\n",
      "Epoch 82/300\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.4042 - acc: 0.8125 - val_loss: 0.3120 - val_acc: 0.8500\n",
      "Epoch 83/300\n",
      "80/80 [==============================] - 0s 0us/step - loss: 0.4034 - acc: 0.8125 - val_loss: 0.3119 - val_acc: 0.8500\n",
      "Epoch 84/300\n",
      "80/80 [==============================] - 0s 189us/step - loss: 0.4030 - acc: 0.8125 - val_loss: 0.3112 - val_acc: 0.8500\n",
      "Epoch 85/300\n",
      "80/80 [==============================] - 0s 0us/step - loss: 0.4023 - acc: 0.8125 - val_loss: 0.3109 - val_acc: 0.8500\n",
      "Epoch 86/300\n",
      "80/80 [==============================] - 0s 202us/step - loss: 0.4021 - acc: 0.8125 - val_loss: 0.3106 - val_acc: 0.8500\n",
      "Epoch 87/300\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.4014 - acc: 0.8125 - val_loss: 0.3096 - val_acc: 0.8500\n",
      "Epoch 88/300\n",
      "80/80 [==============================] - 0s 0us/step - loss: 0.4009 - acc: 0.8125 - val_loss: 0.3097 - val_acc: 0.8500\n",
      "Epoch 89/300\n",
      "80/80 [==============================] - 0s 202us/step - loss: 0.4001 - acc: 0.8250 - val_loss: 0.3091 - val_acc: 0.8500\n",
      "Epoch 90/300\n",
      "80/80 [==============================] - 0s 12us/step - loss: 0.4001 - acc: 0.8250 - val_loss: 0.3097 - val_acc: 0.8500\n",
      "Epoch 91/300\n",
      "80/80 [==============================] - 0s 0us/step - loss: 0.3998 - acc: 0.8250 - val_loss: 0.3084 - val_acc: 0.8500\n",
      "Epoch 92/300\n",
      "80/80 [==============================] - 0s 209us/step - loss: 0.3989 - acc: 0.8250 - val_loss: 0.3077 - val_acc: 0.8500\n",
      "Epoch 93/300\n",
      "80/80 [==============================] - 0s 120us/step - loss: 0.3988 - acc: 0.8250 - val_loss: 0.3084 - val_acc: 0.8500\n",
      "Epoch 94/300\n",
      "80/80 [==============================] - 0s 0us/step - loss: 0.3979 - acc: 0.8250 - val_loss: 0.3077 - val_acc: 0.8500\n",
      "Epoch 95/300\n",
      "80/80 [==============================] - 0s 0us/step - loss: 0.3975 - acc: 0.8250 - val_loss: 0.3073 - val_acc: 0.8500\n",
      "Epoch 96/300\n",
      "80/80 [==============================] - 0s 12us/step - loss: 0.3968 - acc: 0.8250 - val_loss: 0.3071 - val_acc: 0.8500\n",
      "Epoch 97/300\n",
      "80/80 [==============================] - 0s 145us/step - loss: 0.3966 - acc: 0.8250 - val_loss: 0.3061 - val_acc: 0.8500\n",
      "Epoch 98/300\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.3960 - acc: 0.8250 - val_loss: 0.3058 - val_acc: 0.8500\n",
      "Epoch 99/300\n",
      "80/80 [==============================] - 0s 90us/step - loss: 0.3956 - acc: 0.8250 - val_loss: 0.3057 - val_acc: 0.8500\n",
      "Epoch 100/300\n",
      "80/80 [==============================] - 0s 73us/step - loss: 0.3950 - acc: 0.8250 - val_loss: 0.3055 - val_acc: 0.8500\n",
      "Epoch 101/300\n",
      "80/80 [==============================] - 0s 0us/step - loss: 0.3948 - acc: 0.8250 - val_loss: 0.3056 - val_acc: 0.8500\n",
      "Epoch 102/300\n",
      "80/80 [==============================] - 0s 239us/step - loss: 0.3941 - acc: 0.8250 - val_loss: 0.3054 - val_acc: 0.8500\n",
      "Epoch 103/300\n",
      "80/80 [==============================] - 0s 0us/step - loss: 0.3937 - acc: 0.8250 - val_loss: 0.3045 - val_acc: 0.8500\n",
      "Epoch 104/300\n",
      "80/80 [==============================] - 0s 223us/step - loss: 0.3936 - acc: 0.8250 - val_loss: 0.3048 - val_acc: 0.8500\n",
      "Epoch 105/300\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.3927 - acc: 0.8250 - val_loss: 0.3044 - val_acc: 0.8500\n",
      "Epoch 106/300\n",
      "80/80 [==============================] - 0s 173us/step - loss: 0.3921 - acc: 0.8250 - val_loss: 0.3034 - val_acc: 0.8500\n",
      "Epoch 107/300\n",
      "80/80 [==============================] - 0s 130us/step - loss: 0.3917 - acc: 0.8250 - val_loss: 0.3029 - val_acc: 0.8500\n",
      "Epoch 108/300\n",
      "80/80 [==============================] - 0s 131us/step - loss: 0.3913 - acc: 0.8250 - val_loss: 0.3022 - val_acc: 0.9000\n",
      "Epoch 109/300\n",
      "80/80 [==============================] - 0s 97us/step - loss: 0.3910 - acc: 0.8250 - val_loss: 0.3020 - val_acc: 0.8500\n",
      "Epoch 110/300\n",
      "80/80 [==============================] - 0s 141us/step - loss: 0.3908 - acc: 0.8250 - val_loss: 0.3018 - val_acc: 0.8500\n",
      "Epoch 111/300\n",
      "80/80 [==============================] - 0s 50us/step - loss: 0.3901 - acc: 0.8250 - val_loss: 0.3017 - val_acc: 0.8500\n",
      "Epoch 112/300\n",
      "80/80 [==============================] - 0s 169us/step - loss: 0.3901 - acc: 0.8250 - val_loss: 0.3010 - val_acc: 0.8500\n",
      "Epoch 113/300\n",
      "80/80 [==============================] - 0s 13us/step - loss: 0.3893 - acc: 0.8250 - val_loss: 0.3008 - val_acc: 0.9000\n",
      "Epoch 114/300\n",
      "80/80 [==============================] - 0s 0us/step - loss: 0.3889 - acc: 0.8250 - val_loss: 0.2998 - val_acc: 0.9000\n",
      "Epoch 115/300\n",
      "80/80 [==============================] - 0s 234us/step - loss: 0.3887 - acc: 0.8250 - val_loss: 0.3002 - val_acc: 0.9000\n",
      "Epoch 116/300\n",
      "80/80 [==============================] - 0s 134us/step - loss: 0.3878 - acc: 0.8250 - val_loss: 0.2997 - val_acc: 0.9000\n",
      "Epoch 117/300\n",
      "80/80 [==============================] - 0s 93us/step - loss: 0.3874 - acc: 0.8250 - val_loss: 0.2990 - val_acc: 0.9000\n",
      "Epoch 118/300\n",
      "80/80 [==============================] - 0s 129us/step - loss: 0.3868 - acc: 0.8250 - val_loss: 0.2991 - val_acc: 0.9000\n",
      "Epoch 119/300\n",
      "80/80 [==============================] - 0s 66us/step - loss: 0.3864 - acc: 0.8250 - val_loss: 0.2986 - val_acc: 0.9000\n",
      "Epoch 120/300\n",
      "80/80 [==============================] - 0s 132us/step - loss: 0.3862 - acc: 0.8250 - val_loss: 0.2986 - val_acc: 0.9000\n",
      "Epoch 121/300\n",
      "80/80 [==============================] - 0s 64us/step - loss: 0.3854 - acc: 0.8375 - val_loss: 0.2983 - val_acc: 0.9000\n",
      "Epoch 122/300\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.3855 - acc: 0.8375 - val_loss: 0.2979 - val_acc: 0.9000\n",
      "Epoch 123/300\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.3847 - acc: 0.8375 - val_loss: 0.2972 - val_acc: 0.9000\n",
      "Epoch 124/300\n",
      "80/80 [==============================] - 0s 94us/step - loss: 0.3845 - acc: 0.8375 - val_loss: 0.2965 - val_acc: 0.9000\n",
      "Epoch 125/300\n",
      "80/80 [==============================] - 0s 73us/step - loss: 0.3839 - acc: 0.8375 - val_loss: 0.2961 - val_acc: 0.9000\n",
      "Epoch 126/300\n",
      "80/80 [==============================] - 0s 0us/step - loss: 0.3834 - acc: 0.8375 - val_loss: 0.2965 - val_acc: 0.9000\n",
      "Epoch 127/300\n",
      "80/80 [==============================] - 0s 216us/step - loss: 0.3833 - acc: 0.8375 - val_loss: 0.2970 - val_acc: 0.9000\n",
      "Epoch 128/300\n",
      "80/80 [==============================] - 0s 125us/step - loss: 0.3826 - acc: 0.8375 - val_loss: 0.2968 - val_acc: 0.9000\n",
      "Epoch 129/300\n",
      "80/80 [==============================] - 0s 37us/step - loss: 0.3820 - acc: 0.8375 - val_loss: 0.2962 - val_acc: 0.9000\n",
      "Epoch 130/300\n",
      "80/80 [==============================] - 0s 207us/step - loss: 0.3818 - acc: 0.8375 - val_loss: 0.2960 - val_acc: 0.9000\n",
      "Epoch 131/300\n",
      "80/80 [==============================] - 0s 70us/step - loss: 0.3816 - acc: 0.8375 - val_loss: 0.2953 - val_acc: 0.9000\n",
      "Epoch 132/300\n",
      "80/80 [==============================] - 0s 146us/step - loss: 0.3807 - acc: 0.8375 - val_loss: 0.2946 - val_acc: 0.9000\n",
      "Epoch 133/300\n",
      "80/80 [==============================] - 0s 13us/step - loss: 0.3808 - acc: 0.8375 - val_loss: 0.2946 - val_acc: 0.9000\n",
      "Epoch 134/300\n",
      "80/80 [==============================] - 0s 0us/step - loss: 0.3800 - acc: 0.8375 - val_loss: 0.2943 - val_acc: 0.9000\n",
      "Epoch 135/300\n",
      "80/80 [==============================] - 0s 264us/step - loss: 0.3795 - acc: 0.8375 - val_loss: 0.2943 - val_acc: 0.9000\n",
      "Epoch 136/300\n",
      "80/80 [==============================] - 0s 101us/step - loss: 0.3790 - acc: 0.8375 - val_loss: 0.2944 - val_acc: 0.9000\n",
      "Epoch 137/300\n",
      "80/80 [==============================] - 0s 116us/step - loss: 0.3791 - acc: 0.8375 - val_loss: 0.2941 - val_acc: 0.9000\n",
      "Epoch 138/300\n",
      "80/80 [==============================] - 0s 124us/step - loss: 0.3782 - acc: 0.8375 - val_loss: 0.2934 - val_acc: 0.9000\n",
      "Epoch 139/300\n",
      "80/80 [==============================] - 0s 42us/step - loss: 0.3776 - acc: 0.8375 - val_loss: 0.2930 - val_acc: 0.9000\n",
      "Epoch 140/300\n",
      "80/80 [==============================] - 0s 138us/step - loss: 0.3772 - acc: 0.8375 - val_loss: 0.2932 - val_acc: 0.9000\n",
      "Epoch 141/300\n",
      "80/80 [==============================] - 0s 50us/step - loss: 0.3767 - acc: 0.8375 - val_loss: 0.2927 - val_acc: 0.9000\n",
      "Epoch 142/300\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.3763 - acc: 0.8375 - val_loss: 0.2925 - val_acc: 0.9000\n",
      "Epoch 143/300\n",
      "80/80 [==============================] - 0s 0us/step - loss: 0.3758 - acc: 0.8375 - val_loss: 0.2924 - val_acc: 0.9000\n",
      "Epoch 144/300\n",
      "80/80 [==============================] - 0s 107us/step - loss: 0.3754 - acc: 0.8375 - val_loss: 0.2919 - val_acc: 0.9000\n",
      "Epoch 145/300\n",
      "80/80 [==============================] - 0s 0us/step - loss: 0.3748 - acc: 0.8375 - val_loss: 0.2916 - val_acc: 0.9000\n",
      "Epoch 146/300\n",
      "80/80 [==============================] - 0s 103us/step - loss: 0.3747 - acc: 0.8375 - val_loss: 0.2910 - val_acc: 0.9000\n",
      "Epoch 147/300\n",
      "80/80 [==============================] - 0s 90us/step - loss: 0.3741 - acc: 0.8375 - val_loss: 0.2909 - val_acc: 0.9000\n",
      "Epoch 148/300\n",
      "80/80 [==============================] - 0s 88us/step - loss: 0.3737 - acc: 0.8375 - val_loss: 0.2906 - val_acc: 0.9000\n",
      "Epoch 149/300\n",
      "80/80 [==============================] - 0s 82us/step - loss: 0.3731 - acc: 0.8375 - val_loss: 0.2899 - val_acc: 0.9000\n",
      "Epoch 150/300\n",
      "80/80 [==============================] - 0s 91us/step - loss: 0.3739 - acc: 0.8375 - val_loss: 0.2904 - val_acc: 0.9000\n",
      "Epoch 151/300\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.3722 - acc: 0.8375 - val_loss: 0.2904 - val_acc: 0.9000\n",
      "Epoch 152/300\n",
      "80/80 [==============================] - 0s 38us/step - loss: 0.3721 - acc: 0.8375 - val_loss: 0.2905 - val_acc: 0.9000\n",
      "Epoch 153/300\n",
      "80/80 [==============================] - 0s 142us/step - loss: 0.3714 - acc: 0.8375 - val_loss: 0.2902 - val_acc: 0.9000\n",
      "Epoch 154/300\n",
      "80/80 [==============================] - 0s 86us/step - loss: 0.3710 - acc: 0.8375 - val_loss: 0.2899 - val_acc: 0.9000\n",
      "Epoch 155/300\n",
      "80/80 [==============================] - 0s 112us/step - loss: 0.3703 - acc: 0.8375 - val_loss: 0.2894 - val_acc: 0.9000\n",
      "Epoch 156/300\n",
      "80/80 [==============================] - 0s 97us/step - loss: 0.3700 - acc: 0.8375 - val_loss: 0.2884 - val_acc: 0.9000\n",
      "Epoch 157/300\n",
      "80/80 [==============================] - 0s 80us/step - loss: 0.3694 - acc: 0.8375 - val_loss: 0.2882 - val_acc: 0.9000\n",
      "Epoch 158/300\n",
      "80/80 [==============================] - 0s 129us/step - loss: 0.3695 - acc: 0.8375 - val_loss: 0.2879 - val_acc: 0.9000\n",
      "Epoch 159/300\n",
      "80/80 [==============================] - 0s 92us/step - loss: 0.3690 - acc: 0.8375 - val_loss: 0.2880 - val_acc: 0.9000\n",
      "Epoch 160/300\n",
      "80/80 [==============================] - 0s 125us/step - loss: 0.3679 - acc: 0.8375 - val_loss: 0.2878 - val_acc: 0.9000\n",
      "Epoch 161/300\n",
      "80/80 [==============================] - 0s 113us/step - loss: 0.3675 - acc: 0.8375 - val_loss: 0.2878 - val_acc: 0.9000\n",
      "Epoch 162/300\n",
      "80/80 [==============================] - 0s 103us/step - loss: 0.3672 - acc: 0.8375 - val_loss: 0.2871 - val_acc: 0.9000\n",
      "Epoch 163/300\n",
      "80/80 [==============================] - 0s 93us/step - loss: 0.3666 - acc: 0.8375 - val_loss: 0.2867 - val_acc: 0.9000\n",
      "Epoch 164/300\n",
      "80/80 [==============================] - 0s 105us/step - loss: 0.3661 - acc: 0.8375 - val_loss: 0.2861 - val_acc: 0.9000\n",
      "Epoch 165/300\n",
      "80/80 [==============================] - 0s 161us/step - loss: 0.3659 - acc: 0.8375 - val_loss: 0.2856 - val_acc: 0.9000\n",
      "Epoch 166/300\n",
      "80/80 [==============================] - 0s 99us/step - loss: 0.3653 - acc: 0.8375 - val_loss: 0.2852 - val_acc: 0.9000\n",
      "Epoch 167/300\n",
      "80/80 [==============================] - 0s 88us/step - loss: 0.3648 - acc: 0.8375 - val_loss: 0.2859 - val_acc: 0.9000\n",
      "Epoch 168/300\n",
      "80/80 [==============================] - 0s 141us/step - loss: 0.3647 - acc: 0.8375 - val_loss: 0.2852 - val_acc: 0.9000\n",
      "Epoch 169/300\n",
      "80/80 [==============================] - 0s 122us/step - loss: 0.3639 - acc: 0.8375 - val_loss: 0.2854 - val_acc: 0.9000\n",
      "Epoch 170/300\n",
      "80/80 [==============================] - 0s 101us/step - loss: 0.3637 - acc: 0.8375 - val_loss: 0.2854 - val_acc: 0.9000\n",
      "Epoch 171/300\n",
      "80/80 [==============================] - 0s 76us/step - loss: 0.3630 - acc: 0.8375 - val_loss: 0.2852 - val_acc: 0.9000\n",
      "Epoch 172/300\n",
      "80/80 [==============================] - 0s 196us/step - loss: 0.3628 - acc: 0.8375 - val_loss: 0.2852 - val_acc: 0.9000\n",
      "Epoch 173/300\n",
      "80/80 [==============================] - 0s 94us/step - loss: 0.3622 - acc: 0.8375 - val_loss: 0.2852 - val_acc: 0.9000\n",
      "Epoch 174/300\n",
      "80/80 [==============================] - 0s 77us/step - loss: 0.3618 - acc: 0.8375 - val_loss: 0.2847 - val_acc: 0.9000\n",
      "Epoch 175/300\n",
      "80/80 [==============================] - 0s 176us/step - loss: 0.3617 - acc: 0.8375 - val_loss: 0.2846 - val_acc: 0.9000\n",
      "Epoch 176/300\n",
      "80/80 [==============================] - 0s 85us/step - loss: 0.3611 - acc: 0.8375 - val_loss: 0.2847 - val_acc: 0.9000\n",
      "Epoch 177/300\n",
      "80/80 [==============================] - 0s 138us/step - loss: 0.3609 - acc: 0.8375 - val_loss: 0.2840 - val_acc: 0.9000\n",
      "Epoch 178/300\n",
      "80/80 [==============================] - 0s 57us/step - loss: 0.3604 - acc: 0.8375 - val_loss: 0.2835 - val_acc: 0.9000\n",
      "Epoch 179/300\n",
      "80/80 [==============================] - 0s 135us/step - loss: 0.3596 - acc: 0.8375 - val_loss: 0.2830 - val_acc: 0.9000\n",
      "Epoch 180/300\n",
      "80/80 [==============================] - 0s 117us/step - loss: 0.3594 - acc: 0.8375 - val_loss: 0.2825 - val_acc: 0.9000\n",
      "Epoch 181/300\n",
      "80/80 [==============================] - 0s 119us/step - loss: 0.3596 - acc: 0.8375 - val_loss: 0.2833 - val_acc: 0.9000\n",
      "Epoch 182/300\n",
      "80/80 [==============================] - 0s 145us/step - loss: 0.3583 - acc: 0.8375 - val_loss: 0.2827 - val_acc: 0.9000\n",
      "Epoch 183/300\n",
      "80/80 [==============================] - 0s 13us/step - loss: 0.3578 - acc: 0.8375 - val_loss: 0.2827 - val_acc: 0.9000\n",
      "Epoch 184/300\n",
      "80/80 [==============================] - 0s 111us/step - loss: 0.3573 - acc: 0.8375 - val_loss: 0.2829 - val_acc: 0.9000\n",
      "Epoch 185/300\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.3570 - acc: 0.8375 - val_loss: 0.2821 - val_acc: 0.9000\n",
      "Epoch 186/300\n",
      "80/80 [==============================] - 0s 0us/step - loss: 0.3564 - acc: 0.8375 - val_loss: 0.2815 - val_acc: 0.9000\n",
      "Epoch 187/300\n",
      "80/80 [==============================] - 0s 209us/step - loss: 0.3560 - acc: 0.8375 - val_loss: 0.2812 - val_acc: 0.9000\n",
      "Epoch 188/300\n",
      "80/80 [==============================] - 0s 92us/step - loss: 0.3554 - acc: 0.8375 - val_loss: 0.2807 - val_acc: 0.9000\n",
      "Epoch 189/300\n",
      "80/80 [==============================] - 0s 0us/step - loss: 0.3550 - acc: 0.8375 - val_loss: 0.2801 - val_acc: 0.9000\n",
      "Epoch 190/300\n",
      "80/80 [==============================] - 0s 251us/step - loss: 0.3545 - acc: 0.8375 - val_loss: 0.2804 - val_acc: 0.9000\n",
      "Epoch 191/300\n",
      "80/80 [==============================] - 0s 29us/step - loss: 0.3540 - acc: 0.8375 - val_loss: 0.2803 - val_acc: 0.9000\n",
      "Epoch 192/300\n",
      "80/80 [==============================] - 0s 143us/step - loss: 0.3534 - acc: 0.8375 - val_loss: 0.2800 - val_acc: 0.9000\n",
      "Epoch 193/300\n",
      "80/80 [==============================] - 0s 163us/step - loss: 0.3528 - acc: 0.8375 - val_loss: 0.2797 - val_acc: 0.9000\n",
      "Epoch 194/300\n",
      "80/80 [==============================] - 0s 114us/step - loss: 0.3526 - acc: 0.8375 - val_loss: 0.2790 - val_acc: 0.9000\n",
      "Epoch 195/300\n",
      "80/80 [==============================] - 0s 104us/step - loss: 0.3519 - acc: 0.8375 - val_loss: 0.2785 - val_acc: 0.9000\n",
      "Epoch 196/300\n",
      "80/80 [==============================] - 0s 114us/step - loss: 0.3516 - acc: 0.8375 - val_loss: 0.2777 - val_acc: 0.9000\n",
      "Epoch 197/300\n",
      "80/80 [==============================] - 0s 44us/step - loss: 0.3511 - acc: 0.8500 - val_loss: 0.2778 - val_acc: 0.9000\n",
      "Epoch 198/300\n",
      "80/80 [==============================] - 0s 0us/step - loss: 0.3505 - acc: 0.8375 - val_loss: 0.2773 - val_acc: 0.9000\n",
      "Epoch 199/300\n",
      "80/80 [==============================] - 0s 297us/step - loss: 0.3506 - acc: 0.8375 - val_loss: 0.2765 - val_acc: 0.9000\n",
      "Epoch 200/300\n",
      "80/80 [==============================] - 0s 104us/step - loss: 0.3493 - acc: 0.8375 - val_loss: 0.2759 - val_acc: 0.9000\n",
      "Epoch 201/300\n",
      "80/80 [==============================] - 0s 90us/step - loss: 0.3489 - acc: 0.8375 - val_loss: 0.2754 - val_acc: 0.9000\n",
      "Epoch 202/300\n",
      "80/80 [==============================] - 0s 71us/step - loss: 0.3485 - acc: 0.8375 - val_loss: 0.2744 - val_acc: 0.9000\n",
      "Epoch 203/300\n",
      "80/80 [==============================] - 0s 135us/step - loss: 0.3476 - acc: 0.8375 - val_loss: 0.2735 - val_acc: 0.9000\n",
      "Epoch 204/300\n",
      "80/80 [==============================] - 0s 134us/step - loss: 0.3470 - acc: 0.8375 - val_loss: 0.2735 - val_acc: 0.9000\n",
      "Epoch 205/300\n",
      "80/80 [==============================] - 0s 90us/step - loss: 0.3465 - acc: 0.8375 - val_loss: 0.2730 - val_acc: 0.9000\n",
      "Epoch 206/300\n",
      "80/80 [==============================] - 0s 101us/step - loss: 0.3466 - acc: 0.8500 - val_loss: 0.2717 - val_acc: 0.9000\n",
      "Epoch 207/300\n",
      "80/80 [==============================] - 0s 0us/step - loss: 0.3455 - acc: 0.8500 - val_loss: 0.2715 - val_acc: 0.9000\n",
      "Epoch 208/300\n",
      "80/80 [==============================] - 0s 205us/step - loss: 0.3458 - acc: 0.8375 - val_loss: 0.2712 - val_acc: 0.9000\n",
      "Epoch 209/300\n",
      "80/80 [==============================] - 0s 0us/step - loss: 0.3448 - acc: 0.8500 - val_loss: 0.2707 - val_acc: 0.9000\n",
      "Epoch 210/300\n",
      "80/80 [==============================] - 0s 103us/step - loss: 0.3445 - acc: 0.8500 - val_loss: 0.2704 - val_acc: 0.9000\n",
      "Epoch 211/300\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.3443 - acc: 0.8500 - val_loss: 0.2701 - val_acc: 0.9000\n",
      "Epoch 212/300\n",
      "80/80 [==============================] - 0s 29us/step - loss: 0.3433 - acc: 0.8500 - val_loss: 0.2702 - val_acc: 0.9000\n",
      "Epoch 213/300\n",
      "80/80 [==============================] - 0s 102us/step - loss: 0.3431 - acc: 0.8500 - val_loss: 0.2695 - val_acc: 0.9000\n",
      "Epoch 214/300\n",
      "80/80 [==============================] - 0s 27us/step - loss: 0.3425 - acc: 0.8500 - val_loss: 0.2697 - val_acc: 0.9000\n",
      "Epoch 215/300\n",
      "80/80 [==============================] - 0s 84us/step - loss: 0.3419 - acc: 0.8500 - val_loss: 0.2693 - val_acc: 0.9000\n",
      "Epoch 216/300\n",
      "80/80 [==============================] - 0s 106us/step - loss: 0.3418 - acc: 0.8500 - val_loss: 0.2692 - val_acc: 0.9000\n",
      "Epoch 217/300\n",
      "80/80 [==============================] - 0s 148us/step - loss: 0.3411 - acc: 0.8500 - val_loss: 0.2681 - val_acc: 0.9000\n",
      "Epoch 218/300\n",
      "80/80 [==============================] - 0s 12us/step - loss: 0.3413 - acc: 0.8500 - val_loss: 0.2685 - val_acc: 0.9000\n",
      "Epoch 219/300\n",
      "80/80 [==============================] - 0s 137us/step - loss: 0.3402 - acc: 0.8500 - val_loss: 0.2684 - val_acc: 0.9000\n",
      "Epoch 220/300\n",
      "80/80 [==============================] - 0s 174us/step - loss: 0.3398 - acc: 0.8500 - val_loss: 0.2686 - val_acc: 0.9000\n",
      "Epoch 221/300\n",
      "80/80 [==============================] - 0s 77us/step - loss: 0.3395 - acc: 0.8500 - val_loss: 0.2685 - val_acc: 0.9000\n",
      "Epoch 222/300\n",
      "80/80 [==============================] - 0s 0us/step - loss: 0.3389 - acc: 0.8500 - val_loss: 0.2674 - val_acc: 0.9000\n",
      "Epoch 223/300\n",
      "80/80 [==============================] - 0s 84us/step - loss: 0.3382 - acc: 0.8500 - val_loss: 0.2678 - val_acc: 0.9000\n",
      "Epoch 224/300\n",
      "80/80 [==============================] - 0s 111us/step - loss: 0.3381 - acc: 0.8500 - val_loss: 0.2672 - val_acc: 0.9000\n",
      "Epoch 225/300\n",
      "80/80 [==============================] - 0s 88us/step - loss: 0.3374 - acc: 0.8500 - val_loss: 0.2670 - val_acc: 0.9000\n",
      "Epoch 226/300\n",
      "80/80 [==============================] - 0s 118us/step - loss: 0.3367 - acc: 0.8500 - val_loss: 0.2669 - val_acc: 0.9000\n",
      "Epoch 227/300\n",
      "80/80 [==============================] - 0s 120us/step - loss: 0.3362 - acc: 0.8500 - val_loss: 0.2664 - val_acc: 0.9000\n",
      "Epoch 228/300\n",
      "80/80 [==============================] - 0s 125us/step - loss: 0.3359 - acc: 0.8500 - val_loss: 0.2663 - val_acc: 0.9000\n",
      "Epoch 229/300\n",
      "80/80 [==============================] - 0s 74us/step - loss: 0.3355 - acc: 0.8500 - val_loss: 0.2656 - val_acc: 0.9000\n",
      "Epoch 230/300\n",
      "80/80 [==============================] - 0s 91us/step - loss: 0.3349 - acc: 0.8500 - val_loss: 0.2663 - val_acc: 0.9000\n",
      "Epoch 231/300\n",
      "80/80 [==============================] - 0s 160us/step - loss: 0.3347 - acc: 0.8500 - val_loss: 0.2660 - val_acc: 0.9000\n",
      "Epoch 232/300\n",
      "80/80 [==============================] - 0s 94us/step - loss: 0.3343 - acc: 0.8500 - val_loss: 0.2659 - val_acc: 0.9000\n",
      "Epoch 233/300\n",
      "80/80 [==============================] - 0s 127us/step - loss: 0.3336 - acc: 0.8500 - val_loss: 0.2660 - val_acc: 0.9000\n",
      "Epoch 234/300\n",
      "80/80 [==============================] - 0s 98us/step - loss: 0.3331 - acc: 0.8500 - val_loss: 0.2658 - val_acc: 0.9000\n",
      "Epoch 235/300\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.3324 - acc: 0.8500 - val_loss: 0.2656 - val_acc: 0.9000\n",
      "Epoch 236/300\n",
      "80/80 [==============================] - 0s 49us/step - loss: 0.3320 - acc: 0.8500 - val_loss: 0.2652 - val_acc: 0.9000\n",
      "Epoch 237/300\n",
      "80/80 [==============================] - 0s 158us/step - loss: 0.3319 - acc: 0.8500 - val_loss: 0.2657 - val_acc: 0.9000\n",
      "Epoch 238/300\n",
      "80/80 [==============================] - 0s 143us/step - loss: 0.3312 - acc: 0.8625 - val_loss: 0.2656 - val_acc: 0.9000\n",
      "Epoch 239/300\n",
      "80/80 [==============================] - 0s 76us/step - loss: 0.3316 - acc: 0.8625 - val_loss: 0.2655 - val_acc: 0.9000\n",
      "Epoch 240/300\n",
      "80/80 [==============================] - 0s 0us/step - loss: 0.3306 - acc: 0.8500 - val_loss: 0.2649 - val_acc: 0.9000\n",
      "Epoch 241/300\n",
      "80/80 [==============================] - 0s 276us/step - loss: 0.3301 - acc: 0.8625 - val_loss: 0.2646 - val_acc: 0.9000\n",
      "Epoch 242/300\n",
      "80/80 [==============================] - 0s 95us/step - loss: 0.3294 - acc: 0.8625 - val_loss: 0.2652 - val_acc: 0.9000\n",
      "Epoch 243/300\n",
      "80/80 [==============================] - 0s 76us/step - loss: 0.3293 - acc: 0.8625 - val_loss: 0.2653 - val_acc: 0.9000\n",
      "Epoch 244/300\n",
      "80/80 [==============================] - 0s 0us/step - loss: 0.3284 - acc: 0.8625 - val_loss: 0.2648 - val_acc: 0.9000\n",
      "Epoch 245/300\n",
      "80/80 [==============================] - 0s 188us/step - loss: 0.3281 - acc: 0.8625 - val_loss: 0.2646 - val_acc: 0.9000\n",
      "Epoch 246/300\n",
      "80/80 [==============================] - 0s 163us/step - loss: 0.3279 - acc: 0.8625 - val_loss: 0.2646 - val_acc: 0.8500\n",
      "Epoch 247/300\n",
      "80/80 [==============================] - 0s 46us/step - loss: 0.3273 - acc: 0.8625 - val_loss: 0.2648 - val_acc: 0.8500\n",
      "Epoch 248/300\n",
      "80/80 [==============================] - 0s 167us/step - loss: 0.3265 - acc: 0.8625 - val_loss: 0.2644 - val_acc: 0.8500\n",
      "Epoch 249/300\n",
      "80/80 [==============================] - 0s 97us/step - loss: 0.3261 - acc: 0.8625 - val_loss: 0.2639 - val_acc: 0.8500\n",
      "Epoch 250/300\n",
      "80/80 [==============================] - 0s 140us/step - loss: 0.3256 - acc: 0.8625 - val_loss: 0.2634 - val_acc: 0.8500\n",
      "Epoch 251/300\n",
      "80/80 [==============================] - 0s 109us/step - loss: 0.3250 - acc: 0.8625 - val_loss: 0.2641 - val_acc: 0.8500\n",
      "Epoch 252/300\n",
      "80/80 [==============================] - 0s 102us/step - loss: 0.3246 - acc: 0.8625 - val_loss: 0.2639 - val_acc: 0.8500\n",
      "Epoch 253/300\n",
      "80/80 [==============================] - 0s 118us/step - loss: 0.3240 - acc: 0.8625 - val_loss: 0.2637 - val_acc: 0.8500\n",
      "Epoch 254/300\n",
      "80/80 [==============================] - 0s 61us/step - loss: 0.3237 - acc: 0.8625 - val_loss: 0.2638 - val_acc: 0.8500\n",
      "Epoch 255/300\n",
      "80/80 [==============================] - 0s 72us/step - loss: 0.3228 - acc: 0.8625 - val_loss: 0.2635 - val_acc: 0.8500\n",
      "Epoch 256/300\n",
      "80/80 [==============================] - 0s 0us/step - loss: 0.3225 - acc: 0.8625 - val_loss: 0.2637 - val_acc: 0.8500\n",
      "Epoch 257/300\n",
      "80/80 [==============================] - 0s 209us/step - loss: 0.3219 - acc: 0.8625 - val_loss: 0.2634 - val_acc: 0.8500\n",
      "Epoch 258/300\n",
      "80/80 [==============================] - 0s 121us/step - loss: 0.3215 - acc: 0.8625 - val_loss: 0.2629 - val_acc: 0.8500\n",
      "Epoch 259/300\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.3212 - acc: 0.8625 - val_loss: 0.2629 - val_acc: 0.8500\n",
      "Epoch 260/300\n",
      "80/80 [==============================] - 0s 152us/step - loss: 0.3207 - acc: 0.8625 - val_loss: 0.2626 - val_acc: 0.8500\n",
      "Epoch 261/300\n",
      "80/80 [==============================] - 0s 149us/step - loss: 0.3200 - acc: 0.8625 - val_loss: 0.2625 - val_acc: 0.8500\n",
      "Epoch 262/300\n",
      "80/80 [==============================] - 0s 121us/step - loss: 0.3199 - acc: 0.8625 - val_loss: 0.2630 - val_acc: 0.8500\n",
      "Epoch 263/300\n",
      "80/80 [==============================] - 0s 147us/step - loss: 0.3193 - acc: 0.8625 - val_loss: 0.2628 - val_acc: 0.8500\n",
      "Epoch 264/300\n",
      "80/80 [==============================] - 0s 160us/step - loss: 0.3186 - acc: 0.8625 - val_loss: 0.2627 - val_acc: 0.8500\n",
      "Epoch 265/300\n",
      "80/80 [==============================] - 0s 85us/step - loss: 0.3190 - acc: 0.8625 - val_loss: 0.2633 - val_acc: 0.8500\n",
      "Epoch 266/300\n",
      "80/80 [==============================] - 0s 156us/step - loss: 0.3180 - acc: 0.8625 - val_loss: 0.2622 - val_acc: 0.8500\n",
      "Epoch 267/300\n",
      "80/80 [==============================] - 0s 105us/step - loss: 0.3176 - acc: 0.8625 - val_loss: 0.2614 - val_acc: 0.8500\n",
      "Epoch 268/300\n",
      "80/80 [==============================] - 0s 72us/step - loss: 0.3168 - acc: 0.8625 - val_loss: 0.2611 - val_acc: 0.8500\n",
      "Epoch 269/300\n",
      "80/80 [==============================] - 0s 97us/step - loss: 0.3163 - acc: 0.8625 - val_loss: 0.2612 - val_acc: 0.8500\n",
      "Epoch 270/300\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.3158 - acc: 0.8625 - val_loss: 0.2616 - val_acc: 0.8500\n",
      "Epoch 271/300\n",
      "80/80 [==============================] - 0s 98us/step - loss: 0.3159 - acc: 0.8625 - val_loss: 0.2612 - val_acc: 0.8500\n",
      "Epoch 272/300\n",
      "80/80 [==============================] - 0s 93us/step - loss: 0.3150 - acc: 0.8625 - val_loss: 0.2615 - val_acc: 0.8500\n",
      "Epoch 273/300\n",
      "80/80 [==============================] - 0s 131us/step - loss: 0.3144 - acc: 0.8625 - val_loss: 0.2607 - val_acc: 0.8500\n",
      "Epoch 274/300\n",
      "80/80 [==============================] - 0s 86us/step - loss: 0.3139 - acc: 0.8625 - val_loss: 0.2603 - val_acc: 0.8500\n",
      "Epoch 275/300\n",
      "80/80 [==============================] - 0s 121us/step - loss: 0.3135 - acc: 0.8625 - val_loss: 0.2603 - val_acc: 0.8500\n",
      "Epoch 276/300\n",
      "80/80 [==============================] - 0s 101us/step - loss: 0.3130 - acc: 0.8625 - val_loss: 0.2609 - val_acc: 0.8500\n",
      "Epoch 277/300\n",
      "80/80 [==============================] - 0s 58us/step - loss: 0.3129 - acc: 0.8625 - val_loss: 0.2614 - val_acc: 0.8500\n",
      "Epoch 278/300\n",
      "80/80 [==============================] - 0s 154us/step - loss: 0.3122 - acc: 0.8625 - val_loss: 0.2607 - val_acc: 0.8500\n",
      "Epoch 279/300\n",
      "80/80 [==============================] - 0s 168us/step - loss: 0.3123 - acc: 0.8625 - val_loss: 0.2596 - val_acc: 0.8500\n",
      "Epoch 280/300\n",
      "80/80 [==============================] - 0s 66us/step - loss: 0.3115 - acc: 0.8625 - val_loss: 0.2600 - val_acc: 0.8500\n",
      "Epoch 281/300\n",
      "80/80 [==============================] - 0s 170us/step - loss: 0.3110 - acc: 0.8625 - val_loss: 0.2598 - val_acc: 0.8500\n",
      "Epoch 282/300\n",
      "80/80 [==============================] - 0s 118us/step - loss: 0.3107 - acc: 0.8625 - val_loss: 0.2593 - val_acc: 0.8500\n",
      "Epoch 283/300\n",
      "80/80 [==============================] - 0s 121us/step - loss: 0.3100 - acc: 0.8625 - val_loss: 0.2587 - val_acc: 0.8500\n",
      "Epoch 284/300\n",
      "80/80 [==============================] - 0s 88us/step - loss: 0.3096 - acc: 0.8625 - val_loss: 0.2588 - val_acc: 0.8500\n",
      "Epoch 285/300\n",
      "80/80 [==============================] - 0s 126us/step - loss: 0.3092 - acc: 0.8625 - val_loss: 0.2587 - val_acc: 0.8500\n",
      "Epoch 286/300\n",
      "80/80 [==============================] - 0s 110us/step - loss: 0.3089 - acc: 0.8625 - val_loss: 0.2588 - val_acc: 0.8500\n",
      "Epoch 287/300\n",
      "80/80 [==============================] - 0s 95us/step - loss: 0.3084 - acc: 0.8625 - val_loss: 0.2584 - val_acc: 0.8500\n",
      "Epoch 288/300\n",
      "80/80 [==============================] - 0s 124us/step - loss: 0.3080 - acc: 0.8625 - val_loss: 0.2573 - val_acc: 0.8500\n",
      "Epoch 289/300\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.3083 - acc: 0.8625 - val_loss: 0.2566 - val_acc: 0.8500\n",
      "Epoch 290/300\n",
      "80/80 [==============================] - 0s 99us/step - loss: 0.3065 - acc: 0.8625 - val_loss: 0.2567 - val_acc: 0.8500\n",
      "Epoch 291/300\n",
      "80/80 [==============================] - 0s 115us/step - loss: 0.3064 - acc: 0.8625 - val_loss: 0.2567 - val_acc: 0.8500\n",
      "Epoch 292/300\n",
      "80/80 [==============================] - 0s 125us/step - loss: 0.3060 - acc: 0.8625 - val_loss: 0.2572 - val_acc: 0.8500\n",
      "Epoch 293/300\n",
      "80/80 [==============================] - 0s 112us/step - loss: 0.3062 - acc: 0.8625 - val_loss: 0.2561 - val_acc: 0.8500\n",
      "Epoch 294/300\n",
      "80/80 [==============================] - 0s 96us/step - loss: 0.3062 - acc: 0.8625 - val_loss: 0.2566 - val_acc: 0.8500\n",
      "Epoch 295/300\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.3045 - acc: 0.8750 - val_loss: 0.2563 - val_acc: 0.8500\n",
      "Epoch 296/300\n",
      "80/80 [==============================] - 0s 111us/step - loss: 0.3043 - acc: 0.8625 - val_loss: 0.2556 - val_acc: 0.8500\n",
      "Epoch 297/300\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.3041 - acc: 0.8750 - val_loss: 0.2552 - val_acc: 0.8500\n",
      "Epoch 298/300\n",
      "80/80 [==============================] - 0s 114us/step - loss: 0.3033 - acc: 0.8750 - val_loss: 0.2548 - val_acc: 0.8500\n",
      "Epoch 299/300\n",
      "80/80 [==============================] - 0s 99us/step - loss: 0.3029 - acc: 0.8750 - val_loss: 0.2542 - val_acc: 0.8500\n",
      "Epoch 300/300\n",
      "80/80 [==============================] - 0s 103us/step - loss: 0.3023 - acc: 0.8750 - val_loss: 0.2539 - val_acc: 0.8500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23363edb630>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=12, input_dim=4, activation='relu'))\n",
    "model.add(Dense(units=12, activation='relu'))\n",
    "model.add(Dense(units=1,activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.fit(train_X,train_Y, validation_data=(test_X,test_Y), epochs=300, batch_size=16, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Akurasi test\n",
    "akurasi yang didapat dari data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 38us/step\n",
      "85.00000238418579 %\n"
     ]
    }
   ],
   "source": [
    "acc_test = model.evaluate(test_X,test_Y)\n",
    "print(acc_test[1]*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Akurasi Training\n",
    "akurasi yang didapat dari data training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 26us/step\n",
      "87.5 %\n"
     ]
    }
   ],
   "source": [
    "acc_train = model.evaluate(train_X,train_Y)\n",
    "print(acc_train[1]*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grafik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Loss dan Akurasi\n",
    "    Untuk menampilkan hasil yang telah prosessing dengan menggunakan variable untuk menampung hasil dari prosessing\n",
    "    \n",
    "    -Loss =  untuk menampung loss dari training\n",
    "    -acc = untuk menamping akurasi dari training\n",
    "    -val_loss = untuk menampung loss dari test\n",
    "    -val_acc = untuk menampung akurasi dari test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model.history.history['loss']\n",
    "acc = model.history.history['acc']\n",
    "val_loss = model.history.history['val_loss']\n",
    "val_acc = model.history.history['val_acc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Grafik Loss\n",
    "    Grafik loss yang digunakan adalah perbandingan dari loss data train dan data training dengan grafik merah grafik dari\n",
    "    training dan biru dari data test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2336562f208>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig,(ax0) = plt.subplots(nrows=1, figsize=(10,10))\n",
    "ax0.plot(np.arange(300),loss,'r')\n",
    "ax0.plot(np.arange(300),val_loss,'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data trainig menghasilkan loss data semakin menurun hampir mendekati 0, tetapi pada data test menghasilkan loss yang sedikit lebih besar dibandingkan dengan data training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grafik Akurasi\n",
    "    Grafik akurasi yang digunakan adalah perbandingan dari akurasi data train dan data training dengan grafik merah grafik dari\n",
    "    training dan biru dari data test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x23365887748>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig,(ax0) = plt.subplots(nrows=1, figsize=(10,10))\n",
    "ax0.plot(np.arange(300),acc,'r')\n",
    "ax0.plot(np.arange(300),val_acc,'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berbeda dengan akurasi, kedua data baik itu data training dan data test memiliki akurasi awal yang sama, ketika pada perhitungan +-50 sampai +- 200 data training memiliki akurasi yang besar dari data test dan pada saat epock 250++ data traing mengalami penurunan akurasi dan data test mengalami peningkatan.\n",
    "\n",
    "meskipun begitu akurasi data training tetap lebih besar dari data test\n",
    "\n",
    "data test memiliki akurasi 85% dan data training 87.5%. perbedaan akurasi kedua data tersebut sebanyak 2.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "   Predict merupakan predict dari data test yang akan digunakan untuk menghitung confusion matriks pada data test. data yang digunakan pada variable predict adalah data test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict_classes(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "confusion matriks menggunakan perbandingan antara data test_Y dengan predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matriks\n",
    "Confusion matrix adalah suatu metode yang biasanya digunakan untuk melakukan perhitungan akurasi pada konsep data mining. Rumus ini melakukan perhitungan dengan 4 keluaran, yaitu: recall, precision, acuraccy dan error rate.\n",
    "\n",
    "Confusion Matriks memiliki 4 instilah yaitu :    \n",
    "    \n",
    "1. True Positives (TP): True positif adalah kasus ketika kelas sebenarnya dari titik data adalah 1 (Benar) dan prediksi juga 1 (Benar)\n",
    "\n",
    "2. Negatif Benar (TN): True Negatif adalah kasus ketika kelas sebenarnya dari titik data adalah 0 (Salah) dan prediksi juga 0 (Salah\n",
    "\n",
    "3. Salah Positif (FP): False positif adalah kasus ketika kelas sebenarnya dari titik data adalah 0 (Salah) dan yang diprediksi adalah 1 (Benar).\n",
    "\n",
    "4. Negatif Palsu (FN): False Negatif adalah kasus ketika kelas sebenarnya dari titik data adalah 1 (Benar) dan prediksi adalah 0 (Salah)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "CM = confusion_matrix(test_Y,predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  1],\n",
       "       [ 2, 11]], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Positif\n",
    "TP = CM[[0],[0]]\n",
    "TP = float(TP)\n",
    "\n",
    "# False Positif\n",
    "FP = CM[[0],[1]]\n",
    "FP = float(FP)\n",
    "\n",
    "# False Negatif\n",
    "FN = CM[[1],[0]]\n",
    "FN = float(FN)\n",
    "\n",
    "# True Negatif\n",
    "TN = CM[[1],[1]]\n",
    "TN = float(TN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "akurasi adalah  jumlah prediksi yang tepat yang dibuat oleh model atas semua jenis prediksi yang dibuat.\n",
    "\n",
    "TP + TN / (TP+FP+FN+TN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.0 %\n"
     ]
    }
   ],
   "source": [
    "akurasi = (TP+TN)/(TP+FP+FN+TN)\n",
    "print (akurasi*100,\"%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision\n",
    "\n",
    "TP/(TP + FP)\n",
    "\n",
    "Presisi adalah nilai yang mendefinisikan berapa proporsi pasien yang didiagnosis menderita kanker yang sebenarnya memang menderita kanker. Yang diprediksi mengidap kanker (TP dan FP) dan orang-orang yang benar-benar mengidap kanker (TP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.71428571428571 %\n"
     ]
    }
   ],
   "source": [
    "Precision = TP/(TP+FP)\n",
    "print(Precision*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReCall \n",
    "\n",
    "TP/(TP+FN)\n",
    "\n",
    "Sensitivity adalah nilai yang mendefinisikan berapa proporsi pasien yang benar-benar menderita kanker didiagnosis oleh algoritmik sebagai kanker. Yang sebenarnya mengidap kanker (TP dan FN) dan orang-orang yang didiagnosis oleh model yang mengidap kanker (TP). (Catatan: FN dimasukkan karena Orang yang benar-benar mengidap kanker meskipun modelnya memperkirakan sebaliknya)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.0 %\n"
     ]
    }
   ],
   "source": [
    "Recall = TP/(TP+FN)\n",
    "print(Recall*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specificity\n",
    "\n",
    "TN/(TN+FP)\n",
    "\n",
    "Spesifitas adalah nilai yang mendefinisikan berapa proporsi pasien yang TIDAK menderita kanker, diprediksi oleh model sebagai non-kanker. Yang sebenarnya tidak mengidap kanker(FP dan TN) dan orang-orang yang didiagnosis oleh model tidak menderita kanker adalah TN. (Catatan: FP disertakan karena Orang itu TIDAK benar-benar menderita kanker meskipun modelnya memperkirakan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.66666666666666 %\n"
     ]
    }
   ],
   "source": [
    "Specificity = TN/(TN+FP)\n",
    "print(Specificity*100,\"%\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
